<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Assignment 4: Backpropagation & Optimization - ML Specialization</title>
    
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea, #764ba2);
            min-height: 100vh;
            color: #333;
            line-height: 1.6;
        }

        .header {
            background: rgba(255, 255, 255, 0.95);
            padding: 40px 0;
            text-align: center;
            box-shadow: 0 4px 20px rgba(0,0,0,0.1);
            backdrop-filter: blur(10px);
            margin-bottom: 40px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }

        h1 {
            color: #667eea;
            font-size: 2.8em;
            margin-bottom: 15px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
        }

        .assignment-meta {
            background: linear-gradient(135deg, #ff6b6b, #ffd93d);
            color: white;
            padding: 20px;
            border-radius: 15px;
            margin-bottom: 30px;
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }

        .nav-tabs {
            background: white;
            border-radius: 15px;
            padding: 10px;
            margin-bottom: 30px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
            display: flex;
            flex-wrap: wrap;
            gap: 5px;
        }

        .nav-tab {
            padding: 12px 20px;
            background: #f8f9fa;
            border: none;
            border-radius: 10px;
            cursor: pointer;
            transition: all 0.3s ease;
            font-weight: 500;
            color: #666;
            flex: 1;
            min-width: 140px;
            text-align: center;
        }

        .nav-tab.active {
            background: #667eea;
            color: white;
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(102, 126, 234, 0.3);
        }

        .tab-content {
            display: none;
            background: white;
            border-radius: 20px;
            padding: 40px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.15);
            margin-bottom: 30px;
        }

        .tab-content.active {
            display: block;
        }

        .section-header {
            display: flex;
            align-items: center;
            gap: 15px;
            margin-bottom: 30px;
            padding-bottom: 15px;
            border-bottom: 3px solid #667eea;
        }

        .section-icon {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.5em;
        }

        .code-block {
            background: #2d3748;
            color: #e2e8f0;
            padding: 25px;
            border-radius: 12px;
            margin: 20px 0;
            overflow-x: auto;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            position: relative;
            border-left: 5px solid #667eea;
        }

        .copy-btn {
            position: absolute;
            top: 15px;
            right: 15px;
            background: #667eea;
            color: white;
            border: none;
            padding: 8px 12px;
            border-radius: 6px;
            cursor: pointer;
            font-size: 0.8em;
            transition: background 0.3s ease;
        }

        .copy-btn:hover {
            background: #5a6fd8;
        }

        .objective-card {
            background: linear-gradient(135deg, #f8f9fa, #ffffff);
            border: 2px solid #e9ecef;
            border-radius: 15px;
            padding: 25px;
            margin-bottom: 25px;
            transition: all 0.3s ease;
        }

        .objective-card:hover {
            border-color: #667eea;
            transform: translateY(-3px);
            box-shadow: 0 10px 25px rgba(102, 126, 234, 0.15);
        }

        .task-list {
            background: #f8f9fa;
            border-radius: 12px;
            padding: 25px;
            margin: 20px 0;
        }

        .task-item {
            display: flex;
            align-items: flex-start;
            gap: 15px;
            margin-bottom: 20px;
            padding-bottom: 20px;
            border-bottom: 1px solid #e9ecef;
        }

        .task-item:last-child {
            border-bottom: none;
            margin-bottom: 0;
            padding-bottom: 0;
        }

        .task-number {
            background: #667eea;
            color: white;
            width: 30px;
            height: 30px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
            flex-shrink: 0;
        }

        .math-formula {
            background: #f8f9fa;
            border: 2px solid #e9ecef;
            border-radius: 10px;
            padding: 20px;
            margin: 15px 0;
            text-align: center;
            font-family: 'Times New Roman', serif;
            font-size: 1.1em;
        }

        .optimization-comparison {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .optimizer-card {
            background: white;
            border: 2px solid #e9ecef;
            border-radius: 12px;
            padding: 20px;
            transition: all 0.3s ease;
        }

        .optimizer-card:hover {
            border-color: #667eea;
            transform: translateY(-3px);
            box-shadow: 0 8px 20px rgba(102, 126, 234, 0.15);
        }

        .warning-box {
            background: linear-gradient(135deg, #fff3cd, #ffeaa7);
            border: 1px solid #ffeaa7;
            border-radius: 12px;
            padding: 20px;
            margin: 20px 0;
            display: flex;
            align-items: center;
            gap: 15px;
        }

        .info-box {
            background: linear-gradient(135deg, #d1ecf1, #bee5eb);
            border: 1px solid #bee5eb;
            border-radius: 12px;
            padding: 20px;
            margin: 20px 0;
            display: flex;
            align-items: center;
            gap: 15px;
        }

        .btn {
            background: #667eea;
            color: white;
            padding: 12px 25px;
            text-decoration: none;
            border-radius: 10px;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            font-weight: 600;
            transition: all 0.3s ease;
            border: none;
            cursor: pointer;
            font-size: 1em;
        }

        .btn:hover {
            background: #5a6fd8;
            transform: translateY(-2px);
            box-shadow: 0 8px 20px rgba(102, 126, 234, 0.3);
        }

        @media (max-width: 768px) {
            .container {
                padding: 0 15px;
            }
            
            h1 {
                font-size: 2.2em;
            }
            
            .nav-tabs {
                flex-direction: column;
            }
            
            .nav-tab {
                min-width: auto;
            }
            
            .assignment-meta {
                flex-direction: column;
                text-align: center;
            }
        }
    </style>
</head>
<body>
    <div class="header">
        <div class="container">
            <h1>üîÑ Assignment 4: Backpropagation & Optimization</h1>
            <p>Implement backpropagation algorithm and explore optimization techniques</p>
        </div>
    </div>

    <div class="container">
        <div class="assignment-meta">
            <div>
                <strong>üéØ Difficulty:</strong> Intermediate
            </div>
            <div>
                <strong>‚è±Ô∏è Duration:</strong> 7-9 hours
            </div>
            <div>
                <strong>üõ†Ô∏è Technologies:</strong> NumPy, Calculus, Optimization, Loss Functions
            </div>
            <div>
                <strong>üìä Assignment:</strong> 4/25
            </div>
        </div>

        <div class="nav-tabs">
            <button class="nav-tab active" onclick="showTab('overview')">üìã Overview</button>
            <button class="nav-tab" onclick="showTab('theory')">üßÆ Theory</button>
            <button class="nav-tab" onclick="showTab('setup')">‚öôÔ∏è Setup</button>
            <button class="nav-tab" onclick="showTab('tasks')">üìù Tasks</button>
            <button class="nav-tab" onclick="showTab('code')">üíª Implementation</button>
            <button class="nav-tab" onclick="showTab('evaluation')">üìä Evaluation</button>
        </div>

        <!-- Overview Tab -->
        <div id="overview" class="tab-content active">
            <div class="section-header">
                <div class="section-icon">üìã</div>
                <h2>Assignment Overview</h2>
            </div>

            <div class="objective-card">
                <h3>üéØ Learning Objectives</h3>
                <ul style="margin-top: 15px; margin-left: 20px;">
                    <li>Master the mathematics and implementation of backpropagation</li>
                    <li>Understand gradient computation through computational graphs</li>
                    <li>Implement and compare various optimization algorithms</li>
                    <li>Analyze the impact of different loss functions on training</li>
                    <li>Build a complete trainable neural network from scratch</li>
                </ul>
            </div>

            <div class="objective-card">
                <h3>üè¢ Business Scenario: OptimalAI - Automated Trading System</h3>
                <p><strong>Company:</strong> OptimalAI - A quantitative trading firm specializing in algorithmic trading</p>
                <p><strong>Challenge:</strong> Build neural networks that can learn complex trading patterns and optimize decision-making through advanced gradient-based learning algorithms.</p>
                
                <h4 style="margin-top: 20px;">üìä Trading Challenges:</h4>
                <ul style="margin-top: 10px; margin-left: 20px;">
                    <li><strong>Price Prediction:</strong> Predict future stock prices based on historical data</li>
                    <li><strong>Risk Assessment:</strong> Estimate portfolio risk using neural network models</li>
                    <li><strong>Pattern Recognition:</strong> Identify profitable trading patterns automatically</li>
                    <li><strong>Real-time Learning:</strong> Continuously adapt models to changing market conditions</li>
                </ul>

                <h4 style="margin-top: 20px;">üéØ Technical Requirements:</h4>
                <ul style="margin-top: 10px; margin-left: 20px;">
                    <li>Fast convergence for real-time trading decisions</li>
                    <li>Robust optimization that handles noisy financial data</li>
                    <li>Multiple loss functions for different trading objectives</li>
                    <li>Regularization to prevent overfitting to historical patterns</li>
                </ul>
            </div>

            <div class="warning-box">
                <span style="font-size: 1.5em;">‚ö†Ô∏è</span>
                <div>
                    <strong>Mathematical Intensity:</strong> This assignment requires solid understanding of calculus, particularly chain rule and partial derivatives. You'll implement gradient computation manually and understand the mathematical foundations of neural network training.
                </div>
            </div>
        </div>

        <!-- Theory Tab -->
        <div id="theory" class="tab-content">
            <div class="section-header">
                <div class="section-icon">üßÆ</div>
                <h2>Backpropagation Mathematics</h2>
            </div>

            <div class="task-list">
                <h3>üî¢ Core Mathematical Foundations</h3>
                
                <h4>1. Chain Rule of Calculus</h4>
                <div class="math-formula">
                    <strong>‚àÇL/‚àÇw = (‚àÇL/‚àÇz) √ó (‚àÇz/‚àÇw)</strong><br><br>
                    Where:<br>
                    ‚Ä¢ L = Loss function<br>
                    ‚Ä¢ z = Linear output (w¬∑x + b)<br>
                    ‚Ä¢ w = Weight parameter<br>
                </div>

                <h4>2. Backpropagation Equations</h4>
                <div class="math-formula">
                    <strong>For Output Layer (L):</strong><br>
                    Œ¥^[L] = ‚àÇL/‚àÇa^[L] ‚äô g'^[L](z^[L])<br><br>
                    
                    <strong>For Hidden Layers (l):</strong><br>
                    Œ¥^[l] = (W^[l+1])·µÄ Œ¥^[l+1] ‚äô g'^[l](z^[l])<br><br>
                    
                    <strong>Weight Gradients:</strong><br>
                    ‚àÇL/‚àÇW^[l] = Œ¥^[l] (a^[l-1])·µÄ<br>
                    ‚àÇL/‚àÇb^[l] = Œ¥^[l]
                </div>

                <h4>3. Gradient Descent Update Rules</h4>
                <div class="math-formula">
                    <strong>Standard Gradient Descent:</strong><br>
                    W := W - Œ± √ó ‚àÇL/‚àÇW<br>
                    b := b - Œ± √ó ‚àÇL/‚àÇb<br><br>
                    
                    <strong>Momentum:</strong><br>
                    v := Œ≤ √ó v + (1-Œ≤) √ó ‚àÇL/‚àÇW<br>
                    W := W - Œ± √ó v<br><br>
                    
                    <strong>Adam:</strong><br>
                    m := Œ≤‚ÇÅ √ó m + (1-Œ≤‚ÇÅ) √ó ‚àÇL/‚àÇW<br>
                    v := Œ≤‚ÇÇ √ó v + (1-Œ≤‚ÇÇ) √ó (‚àÇL/‚àÇW)¬≤<br>
                    W := W - Œ± √ó mÃÇ/‚àö(vÃÇ + Œµ)
                </div>

                <h4>4. Loss Function Derivatives</h4>
                <div class="math-formula">
                    <strong>MSE Derivative:</strong><br>
                    ‚àÇL/‚àÇ≈∑ = (≈∑ - y)<br><br>
                    
                    <strong>Binary Cross-Entropy:</strong><br>
                    ‚àÇL/‚àÇ≈∑ = -(y/≈∑) + (1-y)/(1-≈∑)<br><br>
                    
                    <strong>Categorical Cross-Entropy:</strong><br>
                    ‚àÇL/‚àÇ≈∑·µ¢ = -y·µ¢/≈∑·µ¢
                </div>
            </div>

            <div class="optimization-comparison">
                <div class="optimizer-card">
                    <h4>üèÉ SGD</h4>
                    <p><strong>Pros:</strong> Simple, stable, well-understood</p>
                    <p><strong>Cons:</strong> Slow convergence, can get stuck</p>
                    <p><strong>Use Case:</strong> When simplicity is key</p>
                </div>

                <div class="optimizer-card">
                    <h4>üöÄ Momentum</h4>
                    <p><strong>Pros:</strong> Faster convergence, smooths oscillations</p>
                    <p><strong>Cons:</strong> May overshoot minima</p>
                    <p><strong>Use Case:</strong> Smooth optimization landscapes</p>
                </div>

                <div class="optimizer-card">
                    <h4>üìä RMSprop</h4>
                    <p><strong>Pros:</strong> Adapts learning rate per parameter</p>
                    <p><strong>Cons:</strong> Learning rate can become too small</p>
                    <p><strong>Use Case:</strong> Non-stationary objectives</p>
                </div>

                <div class="optimizer-card">
                    <h4>üéØ Adam</h4>
                    <p><strong>Pros:</strong> Combines momentum and adaptive learning</p>
                    <p><strong>Cons:</strong> Complex, may converge to poor solutions</p>
                    <p><strong>Use Case:</strong> Most general-purpose optimizer</p>
                </div>
            </div>
        </div>

        <!-- Setup Tab -->
        <div id="setup" class="tab-content">
            <div class="section-header">
                <div class="section-icon">‚öôÔ∏è</div>
                <h2>Environment Setup</h2>
            </div>

            <div class="task-list">
                <h3>üì¶ Required Libraries</h3>
                <div class="code-block">
                    <button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
                    <pre>
# Core numerical libraries
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# For financial data simulation
import pandas as pd
from datetime import datetime, timedelta

# For data generation and preprocessing
from sklearn.datasets import make_classification, make_regression
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split

# For performance comparison
from sklearn.neural_network import MLPClassifier, MLPRegressor
from sklearn.metrics import accuracy_score, mean_squared_error, classification_report

# For visualization and animation
import matplotlib.animation as animation
from mpl_toolkits.mplot3d import Axes3D

# Math utilities
import math
import time
from collections import defaultdict

# For numerical stability
import warnings
warnings.filterwarnings('ignore')

# Set random seeds for reproducibility
np.random.seed(42)
plt.style.use('seaborn-v0_8')
</pre>
                </div>
            </div>

            <div class="task-list">
                <h3>üîß Installation Commands</h3>
                <div class="code-block">
                    <button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
                    <pre>
# Core packages
pip install numpy matplotlib seaborn pandas scikit-learn

# For enhanced visualizations
pip install plotly

# For financial data simulation
pip install yfinance pandas-datareader

# For optimization visualization
pip install matplotlib

# Optional: For interactive notebooks
pip install jupyter ipywidgets
</pre>
                </div>
            </div>

            <div class="info-box">
                <span style="font-size: 1.5em;">üí°</span>
                <div>
                    <strong>Calculus Review:</strong> Ensure you understand partial derivatives, chain rule, and matrix calculus. These mathematical concepts are fundamental to implementing backpropagation correctly.
                </div>
            </div>
        </div>

        <!-- Tasks Tab -->
        <div id="tasks" class="tab-content">
            <div class="section-header">
                <div class="section-icon">üìù</div>
                <h2>Assignment Tasks</h2>
            </div>

            <div class="task-list">
                <h3>Phase 1: Gradient Computation (2.5 hours)</h3>
                
                <div class="task-item">
                    <div class="task-number">1</div>
                    <div>
                        <h4>Manual Gradient Computation</h4>
                        <p>Implement manual gradient computation for simple functions to understand the chain rule. Start with single-variable functions and progress to multi-variable cases.</p>
                        <strong>Deliverable:</strong> Gradient computation library with test cases
                    </div>
                </div>

                <div class="task-item">
                    <div class="task-number">2</div>
                    <div>
                        <h4>Activation Function Derivatives</h4>
                        <p>Implement derivatives for all activation functions (sigmoid, tanh, ReLU, etc.) and verify them numerically using finite differences.</p>
                        <strong>Deliverable:</strong> Verified activation function derivatives
                    </div>
                </div>

                <div class="task-item">
                    <div class="task-number">3</div>
                    <div>
                        <h4>Loss Function Gradients</h4>
                        <p>Implement gradients for various loss functions and understand how they drive the learning process.</p>
                        <strong>Deliverable:</strong> Loss function gradient implementations with analysis
                    </div>
                </div>
            </div>

            <div class="task-list">
                <h3>Phase 2: Backpropagation Implementation (3 hours)</h3>
                
                <div class="task-item">
                    <div class="task-number">4</div>
                    <div>
                        <h4>Single Layer Backpropagation</h4>
                        <p>Implement backpropagation for a single layer to understand the fundamental mechanics before extending to multi-layer networks.</p>
                        <strong>Deliverable:</strong> Single layer training with gradient verification</strong>
                    </div>
                </div>

                <div class="task-item">
                    <div class="task-number">5</div>
                    <div>
                        <h4>Multi-Layer Backpropagation</h4>
                        <p>Extend backpropagation to handle multiple layers, implementing the recursive gradient computation through the network.</p>
                        <strong>Deliverable:</strong> Complete backpropagation algorithm for deep networks
                    </div>
                </div>

                <div class="task-item">
                    <div class="task-number">6</div>
                    <div>
                        <h4>Batch Processing</h4>
                        <p>Implement efficient batch processing for backpropagation to handle multiple training examples simultaneously.</p>
                        <strong>Deliverable:</strong> Vectorized backpropagation with batch support
                    </div>
                </div>
            </div>

            <div class="task-list">
                <h3>Phase 3: Optimization Algorithms (2.5 hours)</h3>
                
                <div class="task-item">
                    <div class="task-number">7</div>
                    <div>
                        <h4>SGD and Momentum</h4>
                        <p>Implement stochastic gradient descent and momentum-based optimization. Compare convergence behavior on different datasets.</p>
                        <strong>Deliverable:</strong> SGD and Momentum optimizers with convergence analysis
                    </div>
                </div>

                <div class="task-item">
                    <div class="task-number">8</div>
                    <div>
                        <h4>Adaptive Learning Rate Methods</h4>
                        <p>Implement RMSprop and Adam optimizers. Analyze how adaptive learning rates affect training dynamics.</p>
                        <strong>Deliverable:</strong> RMSprop and Adam implementations with comparison study
                    </div>
                </div>

                <div class="task-item">
                    <div class="task-number">9</div>
                    <div>
                        <h4>Learning Rate Scheduling</h4>
                        <p>Implement various learning rate schedules (step decay, exponential decay, cosine annealing) and analyze their impact.</p>
                        <strong>Deliverable:</strong> Learning rate scheduler library with performance analysis
                    </div>
                </div>
            </div>

            <div class="task-list">
                <h3>Phase 4: Trading System Application (1 hour)</h3>
                
                <div class="task-item">
                    <div class="task-number">10</div>
                    <div>
                        <h4>Financial Data Prediction</h4>
                        <p>Apply your trained neural network to predict stock price movements using technical indicators as features.</p>
                        <strong>Deliverable:</strong> Trading signal prediction system with backtesting
                    </div>
                </div>

                <div class="task-item">
                    <div class="task-number">11</div>
                    <div>
                        <h4>Optimization Strategy Analysis</h4>
                        <p>Compare how different optimization algorithms perform on financial prediction tasks and analyze stability.</p>
                        <strong>Deliverable:</strong> Comprehensive optimizer comparison for financial applications
                    </div>
                </div>
            </div>
        </div>

        <!-- Code Implementation Tab -->
        <div id="code" class="tab-content">
            <div class="section-header">
                <div class="section-icon">üíª</div>
                <h2>Implementation</h2>
            </div>

            <div class="task-list">
                <h3>üîÑ Backpropagation Engine</h3>
                <div class="code-block">
                    <button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
                    <pre>
import numpy as np
import matplotlib.pyplot as plt

class BackpropagationEngine:
    """Complete backpropagation implementation with multiple optimizers"""
    
    def __init__(self, layers, activations, learning_rate=0.01, optimizer='sgd'):
        self.layers = layers
        self.num_layers = len(layers)
        self.learning_rate = learning_rate
        self.optimizer = optimizer
        self.activations = activations
        
        # Initialize parameters
        self.weights = {}
        self.biases = {}
        self.initialize_parameters()
        
        # For storing gradients and intermediate values
        self.gradients = {}
        self.z_values = {}
        self.a_values = {}
        
        # For optimization algorithms
        self.momentum_weights = {}
        self.momentum_biases = {}
        self.rmsprop_weights = {}
        self.rmsprop_biases = {}
        self.adam_m_weights = {}
        self.adam_v_weights = {}
        self.adam_m_biases = {}
        self.adam_v_biases = {}
        self.iteration = 0
        
        # Training history
        self.loss_history = []
        self.accuracy_history = []
        
    def initialize_parameters(self, method='xavier'):
        """Initialize weights and biases"""
        for i in range(1, self.num_layers):
            if method == 'xavier':
                self.weights[i] = np.random.randn(self.layers[i], self.layers[i-1]) * np.sqrt(1/self.layers[i-1])
            elif method == 'he':
                self.weights[i] = np.random.randn(self.layers[i], self.layers[i-1]) * np.sqrt(2/self.layers[i-1])
            else:
                self.weights[i] = np.random.randn(self.layers[i], self.layers[i-1]) * 0.01
            
            self.biases[i] = np.zeros((self.layers[i], 1))
            
            # Initialize optimizer states
            self.momentum_weights[i] = np.zeros_like(self.weights[i])
            self.momentum_biases[i] = np.zeros_like(self.biases[i])
            self.rmsprop_weights[i] = np.zeros_like(self.weights[i])
            self.rmsprop_biases[i] = np.zeros_like(self.biases[i])
            self.adam_m_weights[i] = np.zeros_like(self.weights[i])
            self.adam_v_weights[i] = np.zeros_like(self.weights[i])
            self.adam_m_biases[i] = np.zeros_like(self.biases[i])
            self.adam_v_biases[i] = np.zeros_like(self.biases[i])
    
    def activation_function(self, z, activation_type):
        """Apply activation function"""
        if activation_type == 'sigmoid':
            return 1 / (1 + np.exp(-np.clip(z, -500, 500)))
        elif activation_type == 'tanh':
            return np.tanh(z)
        elif activation_type == 'relu':
            return np.maximum(0, z)
        elif activation_type == 'leaky_relu':
            return np.where(z > 0, z, 0.01 * z)
        elif activation_type == 'softmax':
            exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))
            return exp_z / np.sum(exp_z, axis=0, keepdims=True)
        elif activation_type == 'linear':
            return z
        else:
            raise ValueError(f"Unknown activation function: {activation_type}")
    
    def activation_derivative(self, z, activation_type):
        """Compute activation function derivative"""
        if activation_type == 'sigmoid':
            a = self.activation_function(z, 'sigmoid')
            return a * (1 - a)
        elif activation_type == 'tanh':
            return 1 - np.tanh(z)**2
        elif activation_type == 'relu':
            return (z > 0).astype(float)
        elif activation_type == 'leaky_relu':
            return np.where(z > 0, 1, 0.01)
        elif activation_type == 'linear':
            return np.ones_like(z)
        else:
            raise ValueError(f"Unknown activation function: {activation_type}")
    
    def forward_propagation(self, X):
        """Forward propagation through the network"""
        self.a_values[0] = X.T  # Input layer
        
        for i in range(1, self.num_layers):
            # Linear transformation
            self.z_values[i] = np.dot(self.weights[i], self.a_values[i-1]) + self.biases[i]
            
            # Apply activation function
            activation_type = self.activations[i-1]
            self.a_values[i] = self.activation_function(self.z_values[i], activation_type)
        
        return self.a_values[self.num_layers - 1].T
    
    def compute_loss(self, y_true, y_pred, loss_type='mse'):
        """Compute loss function"""
        m = y_true.shape[0]
        
        if loss_type == 'mse':
            return np.mean((y_true - y_pred)**2)
        elif loss_type == 'binary_crossentropy':
            y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)
            return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
        elif loss_type == 'categorical_crossentropy':
            y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)
            return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))
        else:
            raise ValueError(f"Unknown loss function: {loss_type}")
    
    def compute_loss_derivative(self, y_true, y_pred, loss_type='mse'):
        """Compute derivative of loss function"""
        if loss_type == 'mse':
            return y_pred - y_true
        elif loss_type == 'binary_crossentropy':
            y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)
            return -(y_true / y_pred) + (1 - y_true) / (1 - y_pred)
        elif loss_type == 'categorical_crossentropy':
            y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)
            return -y_true / y_pred
        else:
            raise ValueError(f"Unknown loss function: {loss_type}")
    
    def backward_propagation(self, X, y, loss_type='mse'):
        """Backward propagation through the network"""
        m = X.shape[0]
        
        # Get the output
        y_pred = self.forward_propagation(X)
        
        # Compute loss derivative with respect to output
        dA = self.compute_loss_derivative(y, y_pred, loss_type).T / m
        
        # Backward pass through each layer
        for i in range(self.num_layers - 1, 0, -1):
            activation_type = self.activations[i-1]
            
            if activation_type == 'softmax' and loss_type == 'categorical_crossentropy':
                # Special case for softmax + categorical crossentropy
                dZ = (y_pred.T - y.T) / m
            else:
                # General case
                dZ = dA * self.activation_derivative(self.z_values[i], activation_type)
            
            # Compute gradients
            self.gradients[f'dW{i}'] = np.dot(dZ, self.a_values[i-1].T)
            self.gradients[f'db{i}'] = np.sum(dZ, axis=1, keepdims=True)
            
            # Propagate gradient to previous layer
            if i > 1:
                dA = np.dot(self.weights[i].T, dZ)
    
    def update_parameters(self, beta1=0.9, beta2=0.999, epsilon=1e-8):
        """Update parameters using specified optimizer"""
        self.iteration += 1
        
        for i in range(1, self.num_layers):
            dW = self.gradients[f'dW{i}']
            db = self.gradients[f'db{i}']
            
            if self.optimizer == 'sgd':
                self.weights[i] -= self.learning_rate * dW
                self.biases[i] -= self.learning_rate * db
                
            elif self.optimizer == 'momentum':
                self.momentum_weights[i] = beta1 * self.momentum_weights[i] + (1 - beta1) * dW
                self.momentum_biases[i] = beta1 * self.momentum_biases[i] + (1 - beta1) * db
                
                self.weights[i] -= self.learning_rate * self.momentum_weights[i]
                self.biases[i] -= self.learning_rate * self.momentum_biases[i]
                
            elif self.optimizer == 'rmsprop':
                self.rmsprop_weights[i] = beta2 * self.rmsprop_weights[i] + (1 - beta2) * (dW ** 2)
                self.rmsprop_biases[i] = beta2 * self.rmsprop_biases[i] + (1 - beta2) * (db ** 2)
                
                self.weights[i] -= self.learning_rate * dW / (np.sqrt(self.rmsprop_weights[i]) + epsilon)
                self.biases[i] -= self.learning_rate * db / (np.sqrt(self.rmsprop_biases[i]) + epsilon)
                
            elif self.optimizer == 'adam':
                # Update biased first moment estimate
                self.adam_m_weights[i] = beta1 * self.adam_m_weights[i] + (1 - beta1) * dW
                self.adam_m_biases[i] = beta1 * self.adam_m_biases[i] + (1 - beta1) * db
                
                # Update biased second moment estimate
                self.adam_v_weights[i] = beta2 * self.adam_v_weights[i] + (1 - beta2) * (dW ** 2)
                self.adam_v_biases[i] = beta2 * self.adam_v_biases[i] + (1 - beta2) * (db ** 2)
                
                # Bias correction
                m_w_corrected = self.adam_m_weights[i] / (1 - beta1 ** self.iteration)
                m_b_corrected = self.adam_m_biases[i] / (1 - beta1 ** self.iteration)
                v_w_corrected = self.adam_v_weights[i] / (1 - beta2 ** self.iteration)
                v_b_corrected = self.adam_v_biases[i] / (1 - beta2 ** self.iteration)
                
                # Update parameters
                self.weights[i] -= self.learning_rate * m_w_corrected / (np.sqrt(v_w_corrected) + epsilon)
                self.biases[i] -= self.learning_rate * m_b_corrected / (np.sqrt(v_b_corrected) + epsilon)
</pre>
                </div>
            </div>

            <div class="task-list">
                <h3>üéØ Training Loop Implementation</h3>
                <div class="code-block">
                    <button class="copy-btn" onclick="copyToClipboard(this)">Copy</button>
                    <pre>
    def train(self, X, y, epochs=1000, batch_size=32, loss_type='mse', 
              validation_data=None, verbose=True, plot_progress=True):
        """Train the neural network"""
        
        m = X.shape[0]
        self.loss_history = []
        self.accuracy_history = []
        val_loss_history = []
        val_accuracy_history = []
        
        for epoch in range(epochs):
            # Shuffle data
            indices = np.random.permutation(m)
            X_shuffled = X[indices]
            y_shuffled = y[indices]
            
            epoch_loss = 0
            epoch_accuracy = 0
            n_batches = 0
            
            # Mini-batch training
            for i in range(0, m, batch_size):
                X_batch = X_shuffled[i:i+batch_size]
                y_batch = y_shuffled[i:i+batch_size]
                
                # Forward and backward propagation
                y_pred = self.forward_propagation(X_batch)
                self.backward_propagation(X_batch, y_batch, loss_type)
                self.update_parameters()
                
                # Compute batch metrics
                batch_loss = self.compute_loss(y_batch, y_pred, loss_type)
                epoch_loss += batch_loss
                
                # Compute accuracy for classification tasks
                if loss_type in ['binary_crossentropy', 'categorical_crossentropy']:
                    if loss_type == 'binary_crossentropy':
                        predictions = (y_pred > 0.5).astype(int)
                        accuracy = np.mean(predictions == y_batch)
                    else:
                        predictions = np.argmax(y_pred, axis=1)
                        true_labels = np.argmax(y_batch, axis=1)
                        accuracy = np.mean(predictions == true_labels)
                    epoch_accuracy += accuracy
                
                n_batches += 1
            
            # Average metrics for the epoch
            epoch_loss /= n_batches
            epoch_accuracy /= n_batches if n_batches > 0 else 1
            
            self.loss_history.append(epoch_loss)
            self.accuracy_history.append(epoch_accuracy)
            
            # Validation metrics
            if validation_data is not None:
                X_val, y_val = validation_data
                y_val_pred = self.forward_propagation(X_val)
                val_loss = self.compute_loss(y_val, y_val_pred, loss_type)
                val_loss_history.append(val_loss)
                
                if loss_type in ['binary_crossentropy', 'categorical_crossentropy']:
                    if loss_type == 'binary_crossentropy':
                        val_predictions = (y_val_pred > 0.5).astype(int)
                        val_accuracy = np.mean(val_predictions == y_val)
                    else:
                        val_predictions = np.argmax(y_val_pred, axis=1)
                        val_true_labels = np.argmax(y_val, axis=1)
                        val_accuracy = np.mean(val_predictions == val_true_labels)
                    val_accuracy_history.append(val_accuracy)
            
            # Print progress
            if verbose and (epoch + 1) % (epochs // 10) == 0:
                print(f"Epoch {epoch+1}/{epochs}")
                print(f"  Loss: {epoch_loss:.4f}")
                if loss_type in ['binary_crossentropy', 'categorical_crossentropy']:
                    print(f"  Accuracy: {epoch_accuracy:.4f}")
                if validation_data is not None:
                    print(f"  Val Loss: {val_loss:.4f}")
                    if loss_type in ['binary_crossentropy', 'categorical_crossentropy']:
                        print(f"  Val Accuracy: {val_accuracy:.4f}")
                print()
        
        # Plot training progress
        if plot_progress:
            self.plot_training_history(val_loss_history, val_accuracy_history)
    
    def plot_training_history(self, val_loss_history=None, val_accuracy_history=None):
        """Plot training history"""
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))
        
        # Loss plot
        axes[0].plot(self.loss_history, label='Training Loss', color='blue')
        if val_loss_history:
            axes[0].plot(val_loss_history, label='Validation Loss', color='red')
        axes[0].set_title('Training History - Loss')
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('Loss')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # Accuracy plot
        if self.accuracy_history[0] > 0:  # Only plot if accuracy is being tracked
            axes[1].plot(self.accuracy_history, label='Training Accuracy', color='blue')
            if val_accuracy_history:
                axes[1].plot(val_accuracy_history, label='Validation Accuracy', color='red')
            axes[1].set_title('Training History - Accuracy')
            axes[1].set_xlabel('Epoch')
            axes[1].set_ylabel('Accuracy')
            axes[1].legend()
            axes[1].grid(True, alpha=0.3)
        else:
            axes[1].text(0.5, 0.5, 'Accuracy not applicable\nfor regression tasks', 
                        ha='center', va='center', transform=axes[1].transAxes)
            axes[1].set_title('Accuracy Not Applicable')
        
        plt.tight_layout()
        plt.show()
    
    def predict(self, X):
        """Make predictions"""
        output = self.forward_propagation(X)
        
        # For classification tasks, return class predictions
        if self.activations[-1] in ['sigmoid', 'softmax']:
            if output.shape[1] == 1:
                return (output > 0.5).astype(int)
            else:
                return np.argmax(output, axis=1)
        else:
            return output
    
    def predict_proba(self, X):
        """Return prediction probabilities"""
        return self.forward_propagation(X)

# Test the backpropagation implementation
def test_backpropagation():
    """Test backpropagation with classification task"""
    
    # Generate sample data
    from sklearn.datasets import make_classification
    X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, 
                              n_redundant=10, n_clusters_per_class=1, random_state=42)
    
    # Split and standardize data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Reshape labels for binary classification
    y_train = y_train.reshape(-1, 1)
    y_test = y_test.reshape(-1, 1)
    
    # Test different optimizers
    optimizers = ['sgd', 'momentum', 'rmsprop', 'adam']
    results = {}
    
    plt.figure(figsize=(20, 5))
    
    for i, optimizer in enumerate(optimizers):
        print(f"\nTesting {optimizer.upper()} optimizer...")
        
        # Create and train network
        nn = BackpropagationEngine(
            layers=[20, 10, 5, 1], 
            activations=['relu', 'relu', 'sigmoid'],
            learning_rate=0.01,
            optimizer=optimizer
        )
        
        # Train the network
        nn.train(X_train_scaled, y_train, epochs=500, batch_size=32, 
                loss_type='binary_crossentropy', verbose=False, plot_progress=False)
        
        # Make predictions
        y_pred = nn.predict(X_test_scaled)
        accuracy = np.mean(y_pred == y_test)
        results[optimizer] = accuracy
        
        print(f"{optimizer.upper()} Accuracy: {accuracy:.4f}")
        
        # Plot loss history
        plt.subplot(1, 4, i+1)
        plt.plot(nn.loss_history)
        plt.title(f'{optimizer.upper()} Loss History')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # Summary
    print("\n=== OPTIMIZER COMPARISON ===")
    for optimizer, accuracy in results.items():
        print(f"{optimizer.upper()}: {accuracy:.4f}")
    
    return results

# Run the test
test_results = test_backpropagation()
</pre>
                </div>
            </div>
        </div>

        <!-- Evaluation Tab -->
        <div id="evaluation" class="tab-content">
            <div class="section-header">
                <div class="section-icon">üìä</div>
                <h2>Evaluation Criteria</h2>
            </div>

            <div class="task-list">
                <h3>üéØ Technical Implementation (50 points)</h3>
                
                <div class="task-item">
                    <div class="task-number">1</div>
                    <div>
                        <h4>Backpropagation Accuracy (20 points)</h4>
                        <ul>
                            <li><strong>Gradient Computation (8 pts):</strong> Correct implementation of chain rule</li>
                            <li><strong>Multi-layer Support (6 pts):</strong> Proper handling of deep networks</li>
                            <li><strong>Numerical Verification (6 pts):</strong> Gradient checking with finite differences</li>
                        </ul>
                    </div>
                </div>

                <div class="task-item">
                    <div class="task-number">2</div>
                    <div>
                        <h4>Optimization Algorithms (15 points)</h4>
                        <ul>
                            <li><strong>SGD and Momentum (5 pts):</strong> Correct implementation of basic optimizers</li>
                            <li><strong>Adaptive Methods (5 pts):</strong> RMSprop and Adam implementations</li>
                            <li><strong>Learning Rate Scheduling (5 pts):</strong> Various scheduling strategies</li>
                        </ul>
                    </div>
                </div>

                <div class="task-item">
                    <div class="task-number">3</div>
                    <div>
                        <h4>Loss Functions (10 points)</h4>
                        <ul>
                            <li><strong>Implementation (5 pts):</strong> Multiple loss functions with derivatives</li>
                            <li><strong>Numerical Stability (5 pts):</strong> Proper handling of edge cases</li>
                        </ul>
                    </div>
                </div>

                <div class="task-item">
                    <div class="task-number">4</div>
                    <div>
                        <h4>Code Quality (5 points)</h4>
                        <ul>
                            <li>Efficient vectorized operations</li>
                            <li>Clean, modular architecture</li>
                            <li>Comprehensive documentation</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="task-list">
                <h3>üìä Analysis and Understanding (30 points)</h3>
                
                <div class="task-item">
                    <div class="task-number">5</div>
                    <div>
                        <h4>Mathematical Understanding (15 points)</h4>
                        <ul>
                            <li>Clear explanation of backpropagation mathematics</li>
                            <li>Understanding of gradient flow through networks</li>
                            <li>Analysis of optimization algorithm behaviors</li>
                        </ul>
                    </div>
                </div>

                <div class="task-item">
                    <div class="task-number">6</div>
                    <div>
                        <h4>Experimental Analysis (15 points)</h4>
                        <ul>
                            <li>Comprehensive optimizer comparison</li>
                            <li>Convergence behavior analysis</li>
                            <li>Trading system performance evaluation</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="task-list">
                <h3>üìà Business Application (20 points)</h3>
                
                <div class="task-item">
                    <div class="task-number">7</div>
                    <div>
                        <h4>Trading System Implementation (12 points)</h4>
                        <ul>
                            <li>Realistic financial data simulation</li>
                            <li>Appropriate feature engineering</li>
                            <li>Backtesting and performance metrics</li>
                        </ul>
                    </div>
                </div>

                <div class="task-item">
                    <div class="task-number">8</div>
                    <div>
                        <h4>Optimization Strategy (8 points)</h4>
                        <ul>
                            <li>Optimal optimizer selection for financial data</li>
                            <li>Risk-adjusted performance analysis</li>
                            <li>Robustness to market conditions</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="warning-box">
                <span style="font-size: 1.5em;">üìã</span>
                <div>
                    <strong>Submission Requirements:</strong>
                    <ul style="margin-top: 10px; margin-left: 20px;">
                        <li>Complete backpropagation implementation (Jupyter notebook)</li>
                        <li>Optimizer comparison study with visualizations</li>
                        <li>Trading system implementation and backtesting results</li>
                        <li>Mathematical derivations and explanations</li>
                        <li>Performance analysis report</li>
                    </ul>
                </div>
            </div>
        </div>

        <div style="text-align: center; margin-top: 40px;">
            <a href="ml_practical_assignment_3.html" class="btn">
                ‚Üê Previous: Neural Networks
            </a>
            <a href="ml_specialization_assignments_index.html" class="btn" style="margin: 0 15px;">
                Back to All Assignments
            </a>
            <a href="ml_practical_assignment_5.html" class="btn">
                Next: PyTorch/Keras MLP ‚Üí
            </a>
        </div>
    </div>

    <script>
        function showTab(tabName) {
            // Hide all tab contents
            const tabContents = document.querySelectorAll('.tab-content');
            tabContents.forEach(tab => tab.classList.remove('active'));
            
            // Remove active class from all tabs
            const tabs = document.querySelectorAll('.nav-tab');
            tabs.forEach(tab => tab.classList.remove('active'));
            
            // Show selected tab and mark as active
            document.getElementById(tabName).classList.add('active');
            event.target.classList.add('active');
        }

        function copyToClipboard(button) {
            const codeBlock = button.parentNode;
            const code = codeBlock.querySelector('pre').textContent;
            
            navigator.clipboard.writeText(code).then(() => {
                const originalText = button.textContent;
                button.textContent = 'Copied!';
                button.style.background = '#28a745';
                
                setTimeout(() => {
                    button.textContent = originalText;
                    button.style.background = '#667eea';
                }, 2000);
            });
        }
    </script>
</body>
</html>