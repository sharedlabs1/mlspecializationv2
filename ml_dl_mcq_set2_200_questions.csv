Question,Scenario,OptionA,OptionB,OptionC,OptionD,CorrectAnswer,Explanation,Topic
"A company wants to build a chatbot for customer service. They have conversation logs but no labeled intent data. What's the best approach to get started?","Customer service logs contain thousands of conversations like 'I want to return my order', 'Where is my package?', 'Cancel my subscription'. Need to identify common intents.","A) Manually label all conversations first","B) Use unsupervised clustering to discover intent patterns, then label representative samples","C) Build rule-based system only","D) Use pre-trained model without customization","B","Start with unsupervised clustering (K-means, topic modeling) to discover natural intent groupings, then label representative samples from each cluster for training a supervised classifier.","NLP Basics"
"For preprocessing text data before feeding to an NLP model, which steps are typically most important?","Raw customer reviews: 'AMAZING product!!! Best purchase ever ðŸ˜€ #happy' vs 'terrible quality... wouldn't recommend to anyone.'","A) Only lowercase conversion","B) Tokenization, normalization (lowercase, punctuation), handling special characters and emojis","C) Remove all punctuation and numbers","D) No preprocessing needed","B","Essential preprocessing includes: tokenization (split into words/subwords), normalization (lowercase, handle punctuation), emoji handling, and removing/normalizing special characters while preserving meaning.","Data Preprocessing"
"In text classification, what's the difference between bag-of-words (BoW) and TF-IDF representations?","Classifying movie reviews: BoW counts word occurrences, TF-IDF weights words by frequency and rarity across the corpus.","A) BoW and TF-IDF are identical","B) BoW counts word frequency, TF-IDF weights by both frequency and inverse document frequency","C) TF-IDF only counts unique words","D) BoW is always better than TF-IDF","B","BoW represents text as word frequency counts. TF-IDF weights words by their frequency in the document (TF) and inverse frequency across corpus (IDF), emphasizing rare but informative words.","Text Representation"
"When using word embeddings for text classification, what advantage do they provide over traditional sparse representations?","Sparse representation: 'good movie' and 'excellent film' have no shared features. Dense embeddings capture semantic similarity between 'good/excellent' and 'movie/film'.","A) Embeddings are always larger","B) Embeddings capture semantic similarity and reduce dimensionality compared to sparse vectors","C) Sparse representations are always better","D) No significant difference","B","Word embeddings provide dense, low-dimensional representations that capture semantic relationships (synonyms, analogies) that sparse BoW/TF-IDF representations miss.","Word Embeddings"
"In named entity recognition (NER), what types of entities are commonly identified and why is context important?","Sentence: 'Apple released the iPhone in California.' Need to identify 'Apple' (company), 'iPhone' (product), 'California' (location) based on context.","A) Only person names matter","B) Common entities include persons, organizations, locations, dates; context disambiguates entity types","C) Entity type doesn't matter","D) Single words are always sufficient","B","NER identifies entities like PERSON, ORGANIZATION, LOCATION, DATE, MONEY. Context is crucial for disambiguation: 'Apple' could be fruit or company depending on surrounding words.","Named Entity Recognition"
"What is part-of-speech (POS) tagging and how does it help in NLP tasks?","Sentence: 'The bear can bear the cold.' First 'bear' is noun (animal), second 'bear' is verb (endure). POS tagging disambiguates word meanings.","A) POS tagging is unnecessary","B) POS tagging identifies grammatical roles (noun, verb, adjective) which helps disambiguation and parsing","C) POS tags are always the same for identical words","D) Only sentence length matters","B","POS tagging assigns grammatical categories (noun, verb, adjective) to words, helping with disambiguation, parsing, and feature extraction for downstream NLP tasks.","POS Tagging"
"In sentiment analysis, what are the main challenges when dealing with informal text like social media posts?","Tweet: 'OMG this movie is sooooo bad it's actually good lol ðŸ˜‚ #irony' Contains sarcasm, slang, elongated words, emojis, and contradictory sentiment.","A) Informal text has no special challenges","B) Challenges include sarcasm, slang, typos, emojis, abbreviations, and context-dependent sentiment","C) Only grammar matters","D) Sentiment is always explicit","B","Informal text challenges include: sarcasm/irony detection, slang and abbreviations, typos and elongated words, emoji interpretation, and implicit sentiment requiring context understanding.","Sentiment Analysis"
"What is the difference between document-level and aspect-based sentiment analysis?","Restaurant review: 'The food was amazing but the service was terrible and the atmosphere was okay.' Overall sentiment vs. sentiment for specific aspects.","A) Both are identical","B) Document-level gives overall sentiment, aspect-based identifies sentiment for specific aspects/features","C) Aspect-based is always better","D) Only document-level sentiment exists","B","Document-level sentiment provides overall opinion. Aspect-based sentiment analysis identifies opinions about specific aspects (food, service, price) within the same document.","Sentiment Analysis"
"In machine translation, what is the alignment problem and why is it challenging?","Translating 'The quick brown fox' to 'Le renard brun rapide' - word order changes and 'quick' aligns to 'rapide' which appears later in the French sentence.","A) Alignment is not important","B) Alignment maps words/phrases between source and target languages, challenging due to different word orders and idioms","C) Word order is always preserved","D) One-to-one mapping always exists","B","Alignment maps corresponding words/phrases between languages. Challenges include different word orders, one-to-many mappings, idioms, and structural differences between languages.","Machine Translation"
"What are n-grams and how are they used in language modeling and text processing?","Analyzing 'The cat sat on the mat': Unigrams: [The, cat, sat, on, the, mat], Bigrams: [The cat, cat sat, sat on, on the, the mat].","A) N-grams are only single words","B) N-grams are sequences of n consecutive words/tokens used for context modeling and pattern recognition","C) N-grams don't capture context","D) Only bigrams are useful","B","N-grams are sequences of n consecutive tokens. Unigrams (1-word), bigrams (2-word), trigrams (3-word) capture local context patterns for language modeling and feature extraction.","N-grams"
"In information retrieval, what is the difference between precision and recall?","Search for 'machine learning papers': Retrieved 100 papers, 80 relevant. Database contains 200 relevant papers total. Precision=80/100=80%, Recall=80/200=40%.","A) Precision and recall are the same","B) Precision measures relevance of retrieved results, recall measures completeness of retrieval","C) Only precision matters","D) Only recall matters","B","Precision = relevant retrieved / total retrieved (quality of results). Recall = relevant retrieved / total relevant (completeness of retrieval). Both are important for IR evaluation.","Information Retrieval"
"What is text summarization and what are the main approaches?","Long news article about climate change needs a 3-sentence summary. Can either extract key sentences or generate new summary text.","A) Only extractive summarization exists","B) Extractive selects important sentences, abstractive generates new summary text","C) Summarization is always automatic","D) Length doesn't matter in summarization","B","Extractive summarization selects and combines important sentences from source text. Abstractive summarization generates new text that captures key information, similar to human summarization.","Text Summarization"
"In topic modeling, what does Latent Dirichlet Allocation (LDA) discover?","Analyzing news articles: LDA might discover topics like 'politics' (words: election, government, policy), 'sports' (words: game, team, score), 'technology' (words: software, computer, innovation).","A) LDA only counts word frequencies","B) LDA discovers hidden topics as distributions over words and documents as distributions over topics","C) LDA finds only one topic per document","D) Topics are predefined in LDA","B","LDA assumes documents are mixtures of topics, and topics are distributions over words. It discovers these hidden topics and the topic composition of each document.","Topic Modeling"
"What is dependency parsing and how does it differ from constituency parsing?","Sentence: 'The big dog chased the cat.' Dependency: dogâ†’subjectâ†’chased, catâ†’objectâ†’chased. Constituency: [NP The big dog] [VP chased [NP the cat]].","A) Both parsing methods are identical","B) Dependency shows word relationships, constituency shows phrase structure","C) Only dependency parsing is useful","D) Parsing is unnecessary for NLP","B","Dependency parsing identifies grammatical relationships between words (subject, object, modifier). Constituency parsing groups words into phrases and shows hierarchical sentence structure.","Syntactic Parsing"
"In text generation, what is the difference between greedy decoding and beam search?","Generating text with language model: Greedy always picks highest probability next word. Beam search considers multiple hypotheses to find better overall sequences.","A) Greedy and beam search are identical","B) Greedy picks best next token, beam search explores multiple paths to find better overall sequences","C) Beam search is always worse","D) Only greedy decoding works","B","Greedy decoding selects the highest probability token at each step. Beam search maintains multiple hypotheses and can find better overall sequences by considering future context.","Text Generation"
"What is the attention mechanism in neural machine translation and why is it important?","Translating long sentence: traditional seq2seq compresses entire source into fixed vector. Attention allows decoder to focus on relevant source parts for each target word.","A) Attention is only for computer vision","B) Attention allows models to focus on relevant parts of input sequence when generating each output token","C) Attention makes models slower without benefits","D) Fixed-size representations are always better","B","Attention mechanisms allow models to selectively focus on different parts of the input sequence when producing each output, solving the information bottleneck of fixed-size representations.","Attention Mechanism"
"In the Transformer architecture, what makes it different from RNN-based models for sequence processing?","RNNs process sequences sequentially (word by word). Transformers use self-attention to process all positions simultaneously, enabling parallelization.","A) Transformers are slower than RNNs","B) Transformers process sequences in parallel using self-attention instead of sequential processing","C) RNNs and Transformers are identical","D) Transformers only work for short sequences","B","Transformers replace sequential processing with parallel self-attention, enabling faster training, better long-range dependency modeling, and more efficient computation on modern hardware.","Transformers"
"What is self-attention and how does it help in understanding text relationships?","In 'The animal didn't cross the street because it was too tired', self-attention helps identify that 'it' refers to 'animal' not 'street' by learning attention weights.","A) Self-attention only looks at adjacent words","B) Self-attention allows each word to attend to all other words in the sequence to capture dependencies","C) Self-attention is random","D) Dependencies don't matter in text","B","Self-attention computes attention weights between all word pairs in a sequence, allowing the model to identify relevant context and dependencies regardless of distance.","Transformers"
"In multi-head attention, why use multiple attention heads instead of single attention?","Sentence analysis: Head 1 might focus on syntactic relationships (subject-verb), Head 2 on semantic relationships (word meanings), Head 3 on coreference (pronoun resolution).","A) Multiple heads slow down computation","B) Multiple heads allow capturing different types of relationships (syntactic, semantic, positional) simultaneously","C) Single head is always sufficient","D) Multiple heads are just for regularization","B","Multiple attention heads allow the model to attend to different representation subspaces simultaneously, capturing various types of relationships (syntax, semantics, position) in parallel.","Transformers"
"What is the encoder-decoder architecture in Transformers and when is it used?","Machine translation task: Encoder processes source language sentence, decoder generates target language sentence using encoder representations and previously generated tokens.","A) Only decoders are needed","B) Encoder processes input sequence, decoder generates output sequence using encoder representations","C) Encoders and decoders work independently","D) Both encoder and decoder are identical","B","Encoder-decoder architecture: encoder processes input sequence into representations, decoder generates output sequence using encoder representations and autoregressive generation.","Transformers"
"What is BERT's masked language modeling (MLM) training objective?","Training BERT on 'The [MASK] jumped over the fence': BERT uses bidirectional context (words before and after) to predict the masked word.","A) BERT only looks at previous words","B) BERT predicts masked words using bidirectional context (both directions)","C) BERT generates text left-to-right","D) Masking is not important","B","MLM randomly masks tokens and trains BERT to predict them using full bidirectional context, enabling rich bidirectional representations unlike unidirectional language models.","BERT/GPT"
"How does BERT's Next Sentence Prediction (NSP) task work and what does it teach the model?","Training BERT with sentence pairs: 'I love pizza. It's my favorite food.' (IsNext=True) vs 'I love pizza. The weather is nice today.' (IsNext=False).","A) NSP only uses single sentences","B) NSP trains BERT to predict if one sentence logically follows another, learning discourse relationships","C) NSP is unrelated to language understanding","D) NSP makes BERT slower","B","NSP trains BERT to understand relationships between sentences by predicting if sentence B follows sentence A, helping with tasks requiring multi-sentence understanding.","BERT/GPT"
"What is the key difference between BERT and GPT in terms of architecture and training?","BERT uses bidirectional encoder, sees full context when predicting masked words. GPT uses unidirectional decoder, predicts next token seeing only left context.","A) BERT and GPT are identical","B) BERT is bidirectional encoder (MLM), GPT is unidirectional decoder (autoregressive LM)","C) BERT is only for classification","D) GPT cannot understand context","B","BERT: bidirectional encoder trained with masked language modeling. GPT: unidirectional decoder trained with autoregressive language modeling (predicting next tokens).","BERT/GPT"
"When fine-tuning BERT for text classification, what modifications are typically made?","Fine-tuning BERT for sentiment analysis: BERT outputs contextual embeddings for all tokens, but need single sentiment prediction for the entire text.","A) Replace all BERT layers","B) Add classification head on [CLS] token representation","C) Use only the last layer","D) No modifications needed","B","Add a linear classification layer that takes the [CLS] token's final representation (designed to represent the entire sequence) and maps it to class probabilities.","BERT/GPT"
"How does GPT generate text and what is the autoregressive process?","GPT generating story: 'Once upon a time' â†’ predict 'there' â†’ 'Once upon a time there' â†’ predict 'was' â†’ continue until complete story.","A) GPT generates all words simultaneously","B) GPT generates text token by token, using all previous tokens as context for next prediction","C) GPT only copies from training data","D) Generation is random","B","GPT uses autoregressive generation: predict next token given all previous tokens, add predicted token to sequence, repeat. Each prediction uses the full left context.","BERT/GPT"
"What are the main differences between GPT-1, GPT-2, GPT-3, and GPT-4 in terms of capabilities?","GPT evolution: GPT-1 (117M params) basic language modeling, GPT-2 (1.5B) better text generation, GPT-3 (175B) few-shot learning, GPT-4 multimodal capabilities.","A) Only architectural differences","B) Primarily scale and emergent capabilities: larger models show few-shot learning, reasoning, and multimodal abilities","C) All versions are identical","D) Only training data differs","B","Each GPT version shows emergent capabilities with scale: GPT-1 basic LM, GPT-2 coherent generation, GPT-3 few-shot learning and reasoning, GPT-4 multimodal understanding.","BERT/GPT"
"What is few-shot learning in the context of large language models like GPT-3?","GPT-3 can learn new tasks from just a few examples in the prompt: 'English: Hello, French: Bonjour, English: Goodbye, French: Au revoir, English: Thank you, French:'","A) Few-shot requires model retraining","B) Few-shot learns new tasks from examples in the prompt without parameter updates","C) Few-shot is impossible","D) Few-shot needs thousands of examples","B","Few-shot learning enables LLMs to perform new tasks by providing a few examples in the input prompt, without updating model parameters - an emergent capability of large-scale models.","BERT/GPT"
"What is prompt engineering and why is it important for working with large language models?","Different prompts for math problem: 'Solve: 2+3=' vs 'Let's think step by step. What is 2+3? First, I identify this is addition...' The second prompt yields better reasoning.","A) Prompts don't affect model performance","B) Prompt engineering crafts inputs to elicit desired behaviors and improve model performance on specific tasks","C) Only model size matters","D) Random prompts work best","B","Prompt engineering involves carefully crafting input text to guide LLM behavior, improve task performance, and elicit desired reasoning patterns or output formats.","Prompt Engineering"
"What is in-context learning and how does it differ from traditional fine-tuning?","In-context: provide examples in prompt without changing model weights. Fine-tuning: update model parameters on task-specific data. Both adapt models to new tasks.","A) In-context and fine-tuning are identical","B) In-context learning adapts through examples in prompt, fine-tuning updates model parameters","C) Fine-tuning is always better","D) In-context learning requires retraining","B","In-context learning adapts model behavior through examples in the prompt without parameter updates. Fine-tuning updates model parameters on task-specific data.","Prompt Engineering"
"What are some effective prompt engineering techniques for improving LLM performance?","Techniques: Chain-of-thought ('Let's think step by step'), role prompting ('You are an expert mathematician'), few-shot examples, format specification.","A) Only single-word prompts work","B) Effective techniques include chain-of-thought reasoning, role prompting, few-shot examples, and output format specification","C) Prompts should be as short as possible","D) Technical terms should be avoided","B","Effective prompt engineering includes: chain-of-thought reasoning, role-playing prompts, few-shot examples, clear output format specification, and task decomposition.","Prompt Engineering"
"What is chain-of-thought prompting and when is it most beneficial?","Math problem: Instead of 'What is 15% of 80?', use 'Let's solve this step by step: 15% of 80 means 0.15 Ã— 80. First, 0.15 Ã— 80 = 12. Therefore, 15% of 80 is 12.'","A) Chain-of-thought slows down models","B) Chain-of-thought encourages step-by-step reasoning, most beneficial for complex reasoning tasks","C) Only short prompts are effective","D) Reasoning steps are unnecessary","B","Chain-of-thought prompting encourages models to show intermediate reasoning steps, significantly improving performance on complex reasoning, math, and multi-step problem-solving tasks.","Prompt Engineering"
"How do you evaluate the quality of text generated by language models?","Evaluating chatbot responses: need to measure relevance, coherence, factual accuracy, fluency, and appropriateness. Both automatic metrics and human evaluation are important.","A) Only automatic metrics are needed","B) Use combination of automatic metrics (BLEU, ROUGE, perplexity) and human evaluation (relevance, coherence, factuality)","C) Evaluation is unnecessary","D) Only human evaluation matters","B","Text generation evaluation requires: automatic metrics (BLEU, ROUGE, BERTScore) for reference comparison, and human evaluation for relevance, coherence, factuality, and appropriateness.","Model Evaluation"
"What is BLEU score and what does it measure in text generation evaluation?","Machine translation evaluation: comparing generated 'Le chat noir' to reference 'Le chat est noir'. BLEU measures n-gram overlap between generated and reference text.","A) BLEU measures semantic similarity only","B) BLEU measures n-gram precision between generated text and reference translations","C) BLEU evaluates grammatical correctness","D) BLEU is only for classification","B","BLEU (Bilingual Evaluation Understudy) measures n-gram precision between generated text and reference translations, focusing on lexical overlap rather than semantic meaning.","Model Evaluation"
"What are the limitations of automatic metrics like BLEU and ROUGE for text generation?","Two translations: 'The car is red' vs 'The automobile is crimson'. Second is semantically equivalent but BLEU gives low score due to different words.","A) Automatic metrics are perfect","B) Automatic metrics focus on lexical overlap, missing semantic equivalence and coherence","C) Human evaluation is unnecessary","D) Only BLEU scores matter","B","Automatic metrics like BLEU/ROUGE focus on surface-level word overlap, missing semantic equivalence, coherence, fluency, and meaning preservation that humans can evaluate.","Model Evaluation"
"What is the role of human evaluation in assessing language model outputs?","Evaluating creative writing: automatic metrics can't assess creativity, emotional impact, narrative coherence, or cultural appropriateness - requiring human judgment.","A) Human evaluation is too subjective","B) Human evaluation assesses qualities automatic metrics miss: coherence, creativity, appropriateness, factuality","C) Only automatic evaluation is reliable","D) Evaluation is unnecessary","B","Human evaluation captures aspects automatic metrics miss: semantic coherence, factual accuracy, creativity, appropriateness, fluency, and task-specific quality dimensions.","Model Evaluation"
"What is knowledge distillation in the context of language models?","Large teacher model (GPT-3 with 175B parameters) teaches smaller student model (1B parameters) to achieve similar performance with much lower computational cost.","A) Knowledge distillation increases model size","B) Knowledge distillation trains smaller models to mimic larger models' behavior, maintaining performance with lower cost","C) Distillation only works for classification","D) Student models are always worse","B","Knowledge distillation transfers knowledge from large teacher models to smaller student models, achieving similar performance with reduced computational requirements and faster inference.","Model Compression"
"What are the main approaches to reducing the computational cost of large language models?","Deploying 70B parameter model: quantization (reduce precision), pruning (remove weights), distillation (smaller model), efficient attention mechanisms.","A) Computational cost cannot be reduced","B) Approaches include quantization, pruning, distillation, efficient architectures, and specialized hardware","C) Only buying more hardware helps","D) Model compression always hurts performance","B","Cost reduction approaches: quantization (lower precision), pruning (remove parameters), knowledge distillation, efficient attention mechanisms, and optimized hardware utilization.","Model Compression"
"What is model quantization and how does it affect model performance?","Converting 32-bit floating point weights to 8-bit integers: reduces memory by 4x and speeds up inference, but may slightly reduce accuracy due to precision loss.","A) Quantization always improves performance","B) Quantization reduces precision and memory usage, typically with small accuracy trade-offs","C) Quantization only works for small models","D) Precision doesn't affect model quality","B","Quantization reduces numerical precision (32-bit to 8-bit), significantly reducing memory and computation while typically maintaining most of the original model performance.","Model Compression"
"What is neural architecture search (NAS) and how can it help in designing better language models?","Instead of manually designing Transformer variants, NAS automatically explores different architectures: layer types, attention mechanisms, activation functions to find optimal designs.","A) Manual design is always better","B) NAS automatically searches architecture space to find optimal model designs for specific tasks and constraints","C) NAS only works for computer vision","D) Architecture doesn't matter","B","NAS automatically explores architecture design spaces to find optimal model structures for specific tasks, datasets, and computational constraints, often discovering novel architectures.","Neural Architecture Search"
"What are some recent advances in Transformer architecture (like Reformer, Linformer, etc.)?","Standard Transformer attention is O(nÂ²) in sequence length. Reformer uses locality-sensitive hashing, Linformer uses low-rank approximation to reduce complexity.","A) No improvements are possible","B) Recent advances focus on reducing attention complexity, improving efficiency, and handling longer sequences","C) Standard Transformers are optimal","D) Efficiency improvements are unnecessary","B","Recent advances address Transformer limitations: Reformer (LSH attention), Linformer (low-rank attention), Performer (linear attention), BigBird (sparse attention) for efficiency and longer sequences.","Transformers"
"What is the difference between extractive and generative question answering?","Question: 'What is the capital of France?' Extractive QA finds 'Paris' in source text. Generative QA generates answer 'The capital of France is Paris' from knowledge.","A) Both approaches are identical","B) Extractive finds answers within source text, generative creates new answer text","C) Generative is always better","D) Extractive doesn't use source text","B","Extractive QA locates and extracts answer spans from given source text. Generative QA produces answer text using model knowledge, potentially combining multiple sources.","Question Answering"
"In reading comprehension tasks, what makes them challenging for NLP models?","Question: 'Why did John leave?' Text: 'John was late for work. His boss was angry. John decided to find a new job.' Requires causal reasoning and implicit inference.","A) Reading comprehension is always easy","B) Challenges include coreference resolution, implicit reasoning, world knowledge, and multi-sentence understanding","C) Only vocabulary matters","D) Simple pattern matching is sufficient","B","Reading comprehension challenges: coreference resolution, implicit reasoning, causal understanding, world knowledge application, and integration of information across multiple sentences.","Reading Comprehension"
"What is coreference resolution and why is it important in NLP?","Text: 'Sarah bought a car. She loves it because it's efficient.' Need to identify: 'She'â†’Sarah, first 'it'â†’car, second 'it'â†’car.","A) Coreference only applies to pronouns","B) Coreference resolution identifies what pronouns and referring expressions point to in text","C) Coreference is not important","D) Only proper nouns need resolution","B","Coreference resolution identifies which mentions (pronouns, definite descriptions) refer to the same entities, crucial for understanding text meaning and relationships.","Coreference Resolution"
"What are some challenges in multilingual NLP and cross-lingual transfer?","Model trained on English sentiment analysis needs to work on Spanish reviews. Challenges: different vocabularies, syntax, cultural context, and limited labeled data.","A) All languages are identical","B) Challenges include different scripts, syntax, cultural context, and unequal resource availability across languages","C) Translation solves all problems","D) Multilingual models are impossible","B","Multilingual NLP challenges: different writing systems, syntactic structures, cultural contexts, resource imbalances, and the need for cross-lingual knowledge transfer.","Multilingual NLP"
"What is zero-shot cross-lingual transfer and how does it work?","mBERT trained on 104 languages can perform English-trained sentiment classification on Spanish text without seeing Spanish sentiment labels.","A) Zero-shot requires translation","B) Zero-shot applies models trained on one language to other languages without target language training data","C) Zero-shot is impossible","D) All languages need separate training","B","Zero-shot cross-lingual transfer applies models trained on high-resource languages to low-resource languages without target language labeled data, leveraging shared multilingual representations.","Multilingual NLP"
"What are some techniques for handling low-resource languages in NLP?","Building NLP tools for a language with only 1000 speakers and minimal written data. Standard approaches require millions of training examples.","A) Low-resource languages cannot benefit from NLP","B) Techniques include transfer learning, data augmentation, multilingual models, and few-shot learning","C) Only high-resource languages matter","D) Machine translation is the only solution","B","Low-resource NLP techniques: transfer learning from related languages, data augmentation, multilingual pre-trained models, few-shot learning, and active learning strategies.","Low-Resource NLP"
"What is domain adaptation in NLP and when is it needed?","Model trained on news articles performs poorly on medical texts due to specialized vocabulary ('myocardial infarction' vs 'heart attack') and writing styles.","A) Domain adaptation is unnecessary","B) Domain adaptation adjusts models trained on one domain to work effectively on different domains","C) All domains are identical","D) Separate models are always needed","B","Domain adaptation addresses performance degradation when applying models trained on one domain (news) to different domains (medical, legal) with different vocabularies and styles.","Domain Adaptation"
"What are some common techniques for domain adaptation in NLP models?","Adapting news-trained model for medical texts: continued pre-training on medical corpus, fine-tuning with medical labeled data, domain-adversarial training.","A) No adaptation techniques exist","B) Techniques include continued pre-training, fine-tuning, domain-adversarial training, and data augmentation","C) Only retraining from scratch works","D) Domain adaptation is automatic","B","Domain adaptation techniques: continued pre-training on target domain text, fine-tuning with target domain labels, domain-adversarial training, and synthetic data generation.","Domain Adaptation"
"What is active learning and how can it help in building NLP datasets efficiently?","Building sentiment classifier: instead of randomly labeling 10,000 reviews, actively select most informative samples for labeling to achieve better performance with fewer labels.","A) Random sampling is always optimal","B) Active learning selects most informative samples for labeling to maximize model performance with minimal annotation effort","C) More data is always better","D) Active learning only works for classification","B","Active learning strategically selects which samples to label based on model uncertainty or information content, achieving better performance with less labeling effort than random sampling.","Active Learning"
"What are annotation guidelines and why are they important for NLP dataset creation?","Creating named entity dataset: annotators need clear rules for edge cases like 'New York University' (one organization) vs 'University of New York' (potentially different).","A) Annotation guidelines are unnecessary","B) Annotation guidelines ensure consistent, high-quality labeling across annotators and edge cases","C) Annotators always agree naturally","D) Guidelines slow down annotation","B","Annotation guidelines provide clear, consistent rules for labeling decisions, handling edge cases, and ensuring inter-annotator agreement for high-quality dataset creation.","Dataset Creation"
"What is inter-annotator agreement and how is it measured?","Three annotators label sentiment: Annotator A says 'positive', B says 'positive', C says 'neutral'. Need to measure how much they agree on overall dataset.","A) Agreement doesn't matter","B) Inter-annotator agreement measures consistency between annotators, often using Cohen's kappa or Fleiss' kappa","C) Only majority vote matters","D) Perfect agreement is always expected","B","Inter-annotator agreement measures labeling consistency between multiple annotators using metrics like Cohen's kappa (2 annotators) or Fleiss' kappa (multiple annotators).","Dataset Creation"
"What are some strategies for handling annotation disagreements in NLP datasets?","Annotators disagree on sarcasm in 'Great job on being late again.' Some see sarcasm, others see literal praise. How to resolve and use this information?","A) Always use majority vote","B) Strategies include adjudication, multiple labels, confidence scores, and analyzing disagreement patterns","C) Ignore disagreements","D) Remove all disputed examples","B","Disagreement handling strategies: expert adjudication, preserving multiple labels, confidence-weighted labels, analyzing disagreement patterns to identify ambiguous cases.","Dataset Creation"
"What is data augmentation in NLP and what are some common techniques?","Expanding training data for text classification: original 'This movie is great!' â†’ augmented 'This film is excellent!', 'Great movie!', 'This is a great movie!'.","A) Data augmentation is only for computer vision","B) Data augmentation increases training data through transformations like paraphrasing, synonym replacement, and back-translation","C) Original data is always sufficient","D) Augmentation always hurts performance","B","NLP data augmentation techniques: synonym replacement, paraphrasing, back-translation, random insertion/deletion, and adversarial examples to increase training data diversity.","Data Augmentation"
"What is back-translation and how is it used for data augmentation?","Original English: 'I love this product' â†’ translate to Spanish: 'Me encanta este producto' â†’ translate back: 'I love this product' (might become 'I adore this product').","A) Back-translation only works for translation tasks","B) Back-translation creates paraphrases by translating to another language and back, generating diverse training data","C) Back-translation reduces data quality","D) Only forward translation is useful","B","Back-translation translates text to another language and back to create paraphrases, generating diverse training examples while preserving meaning for data augmentation.","Data Augmentation"
"What are adversarial examples in NLP and how do they affect model robustness?","Original: 'This movie is excellent' (predicted: positive). Adversarial: 'This movie is excelent' (typo) (predicted: negative). Small change causes misclassification.","A) Adversarial examples don't exist in NLP","B) Adversarial examples are small input perturbations that cause misclassification, revealing model vulnerabilities","C) Models are naturally robust","D) Spelling doesn't affect NLP models","B","Adversarial examples are carefully crafted inputs with small perturbations (typos, synonyms, word order) that cause model misclassification, exposing robustness issues.","Adversarial Examples"
"How can you improve model robustness against adversarial attacks in NLP?","Defending against adversarial typos and synonym substitutions in sentiment analysis: need techniques to make models more robust to small input changes.","A) Robustness cannot be improved","B) Techniques include adversarial training, data augmentation, ensemble methods, and input preprocessing","C) Adversarial attacks are not a real concern","D) Only larger models are robust","B","Robustness techniques: adversarial training with perturbed examples, data augmentation, ensemble methods, input preprocessing, and regularization techniques.","Adversarial Examples"
"What is curriculum learning and how can it help in training NLP models?","Training sentiment classifier: start with clear examples ('I love this!' â†’ positive), gradually introduce ambiguous cases ('It's okay, I guess' â†’ neutral/negative).","A) Random training order is always best","B) Curriculum learning orders training examples from simple to complex to improve learning efficiency","C) Training order doesn't matter","D) Complex examples should come first","B","Curriculum learning presents training examples in order of increasing difficulty, starting with easy examples and gradually introducing complex cases to improve learning efficiency.","Curriculum Learning"
"What are some ethical considerations in NLP and language model development?","Large language model trained on internet data might contain biases, generate harmful content, perpetuate stereotypes, or be used for misinformation.","A) Ethical considerations don't apply to NLP","B) Key concerns include bias, fairness, privacy, misuse potential, and societal impact of language technologies","C) Technology is inherently neutral","D) Only technical performance matters","B","NLP ethics include: bias and fairness in model outputs, privacy protection, preventing misuse for harmful content generation, and considering societal impact of language technologies.","Ethics in NLP"
"What is bias in NLP models and how can it be measured and mitigated?","Word embeddings show bias: 'programmer' closer to 'man', 'nurse' closer to 'woman'. Language models complete 'The CEO is' with male pronouns more often.","A) Bias doesn't exist in NLP models","B) Bias manifests in unfair associations; measured through word associations and demographic parity; mitigated through debiasing techniques","C) Bias is always beneficial","D) Bias cannot be measured","B","NLP bias appears in word associations and predictions. Measured through word analogy tests and demographic parity. Mitigated through debiasing algorithms and diverse training data.","Ethics in NLP"
"What are some privacy concerns with large language models and how can they be addressed?","Language model might memorize training data like 'My SSN is 123-45-6789' and reproduce it during generation, violating privacy.","A) Privacy is not a concern for language models","B) Concerns include data memorization and reconstruction; addressed through differential privacy and data filtering","C) Language models never memorize data","D) All training data is public","B","Privacy concerns: models may memorize and reproduce sensitive training data. Addressed through differential privacy, careful data curation, and output filtering mechanisms.","Ethics in NLP"