"A retail analyst is using scikit-learn to predict weekly sales. What is the best way to handle categorical features?","The dataset includes 'store_type' and 'region' columns.","A) Drop categorical features","B) One-hot encode categorical features","C) Use label encoding for all features","D) Ignore categorical features","B","One-hot encoding is preferred for non-ordinal categorical features in regression models.","ML Fundamentals"
"A data scientist is evaluating a regression model for house prices. Which metric best penalizes large errors?","The business wants to avoid large prediction mistakes.","A) MAE","B) RMSE","C) R2 Score","D) Accuracy","B","RMSE penalizes large errors more than MAE, making it suitable when large mistakes are costly.","Evaluation Metrics"
"A hospital wants to predict patient readmission risk. What is a key challenge in this ML task?","Readmission events are rare compared to non-readmissions.","A) Class imbalance","B) Feature scaling","C) Overfitting","D) Underfitting","A","Class imbalance is common in medical datasets with rare events.","ML Fundamentals"
"A startup is building a recommendation engine. What is collaborative filtering?","The system suggests products based on user-item interactions.","A) Uses user features only","B) Uses item features only","C) Uses similarities between users and items","D) Ignores user history","C","Collaborative filtering leverages similarities between users and items for recommendations.","Recommendation Systems"
"A bank is using logistic regression for credit approval. What does the output represent?","The model predicts approval (yes/no) for loan applications.","A) Probability of approval","B) Credit score","C) Loan amount","D) Interest rate","A","Logistic regression outputs the probability of the target class (approval).","Classification"
"A telecom company is clustering customers for marketing. What is the elbow method used for?","The team wants to choose the optimal number of clusters.","A) Evaluates cluster separation","B) Determines optimal K","C) Measures feature importance","D) Reduces dimensionality","B","The elbow method helps determine the optimal number of clusters (K) in clustering algorithms.","Clustering"
"A researcher is using PCA for dimensionality reduction. What is a benefit of PCA?","The dataset has 100 features, many of which are correlated.","A) Removes outliers","B) Reduces feature correlation","C) Improves model interpretability","D) All of the above","D","PCA reduces dimensionality, removes correlation, and can improve interpretability.","Dimensionality Reduction"
"A developer is training a neural network with PyTorch. What does optimizer.step() do?","The model is minimizing loss on training data.","A) Computes gradients","B) Updates model weights","C) Calculates loss","D) Loads data","B","optimizer.step() updates model weights using computed gradients.","PyTorch Training"
"A student is building an RNN for time series forecasting. What is a key feature of RNNs?","The model predicts future values based on past sequences.","A) Handles sequential data","B) Ignores order of data","C) Only works for images","D) Requires one-hot encoding","A","RNNs are designed to handle sequential data and capture temporal dependencies.","RNNs"
"A team is tuning dropout rate in a neural network. What is the effect of increasing dropout?","The model is overfitting on training data.","A) Reduces overfitting","B) Increases training speed","C) Improves accuracy","D) No effect","A","Increasing dropout reduces overfitting by randomly dropping neurons during training.","Dropout Regularization"
"A data scientist is using early stopping during training. What is the main benefit?","The model's validation loss starts increasing after several epochs.","A) Prevents overfitting","B) Speeds up training","C) Improves accuracy","D) Reduces memory usage","A","Early stopping halts training when validation loss increases, preventing overfitting.","Early Stopping"
"A developer is using batch normalization in a CNN. What does batch normalization do?","The model has several convolutional layers.","A) Normalizes activations","B) Increases training speed","C) Reduces internal covariate shift","D) All of the above","D","Batch normalization normalizes activations, increases training speed, and reduces covariate shift.","Batch Normalization"
"A researcher is building a sentiment analysis model. What is a common preprocessing step for text?","Text data contains punctuation and stopwords.","A) Remove punctuation","B) Remove stopwords","C) Tokenize text","D) All of the above","D","All listed steps are common for text preprocessing in NLP.","NLP Preprocessing"
"A team is using BERT for text classification. What is a key advantage of BERT?","The model is trained on large text corpora.","A) Bidirectional context","B) Fast inference","C) Low memory usage","D) No pretraining needed","A","BERT uses bidirectional context, improving understanding of text meaning.","BERT"
"A developer is finetuning a HuggingFace transformer. What is a common challenge?","The model is trained on a small dataset.","A) Overfitting","B) Underfitting","C) Slow training","D) No challenge","A","Finetuning on small datasets can lead to overfitting in large models.","HuggingFace Finetuning"
"A data scientist is scraping news articles for NLP. What is a key step after scraping?","Articles contain HTML tags and special characters.","A) Remove HTML tags","B) Tokenize text","C) Remove special characters","D) All of the above","D","All listed steps are important for cleaning scraped text data.","Text Scraping"
"A researcher is using regex to extract phone numbers from text. What is a common regex pattern for phone numbers?","Text data contains various phone number formats.","A) \d{10}","B) [0-9]{10}","C) (\d{3})-\d{3}-\d{4}","D) All of the above","D","All listed patterns can be used to extract phone numbers with regex.","Regex in NLP"
"A developer is quantizing a DL model for mobile deployment. What is a benefit?","The model is deployed on smartphones.","A) Reduced model size","B) Faster inference","C) Lower power consumption","D) All of the above","D","Quantization reduces size, speeds up inference, and lowers power usage.","Quantization"
"A team is using QLoRA for LLM compression. What is a key feature of QLoRA?","The model is trained with limited GPU resources.","A) Low-rank adaptation","B) High memory usage","C) Slow training","D) No advantage","A","QLoRA uses low-rank adaptation for efficient training and compression.","QLoRA"
"A developer is deploying a model with Docker. What is a key benefit?","The model is deployed across different environments.","A) Portability","B) Scalability","C) Isolation","D) All of the above","D","Docker provides portability, scalability, and isolation for deployments.","Docker Deployment"
"A data scientist is embedding documents for search. What is a benefit of embedding?","The system retrieves relevant documents for queries.","A) Faster search","B) Improved accuracy","C) Semantic search","D) All of the above","D","Embeddings enable faster, more accurate, and semantic search.","Document Embedding"
"A team is building a RAG-based retriever for QA. What is a key component?","The retriever combines retrieval and generation for answers.","A) Retriever model","B) Generator model","C) Index","D) All of the above","D","RAG-based retrievers use all listed components for QA tasks.","RAG Retriever"
"A developer is deploying to Azure Blob and using Azure Functions. What is a benefit?","The system processes files automatically.","A) Serverless execution","B) Scalability","C) Cost efficiency","D) All of the above","D","Azure Functions provide serverless execution, scalability, and cost efficiency.","Azure Functions"
"A researcher is coordinating multiple agents for a complex task. What is a challenge?","Agents must communicate and synchronize actions.","A) Communication","B) Synchronization","C) Error handling","D) All of the above","D","Multi-agent systems require communication, synchronization, and error handling.","Agent Coordination"
"A team is testing multi-agent deployment locally. What tool is commonly used?","Agents are built in Python and need orchestration.","A) Docker Compose","B) Kubernetes","C) Jupyter Notebook","D) Excel","A","Docker Compose is used for local multi-agent deployment and orchestration.","Multi-Agent Deployment"
"A developer is running a model on CPU vs GPU. What is a typical result?","The model is a deep neural network for images.","A) GPU is faster for large models","B) CPU is faster for small models","C) Both perform equally","D) GPU is always slower","A","GPUs are optimized for parallel computation and are faster for large models.","CPU vs GPU"
"A researcher is using TensorRT for inference acceleration. What is a benefit?","The model is deployed for real-time inference.","A) Reduced latency","B) Increased throughput","C) Lower memory usage","D) All of the above","D","TensorRT provides reduced latency, increased throughput, and lower memory usage.","TensorRT Acceleration"
"A developer is building a context-driven API block. What is a feature?","The API adapts responses based on user context.","A) Dynamic response generation","B) Static responses","C) No context awareness","D) Only supports GET requests","A","Context-driven APIs generate dynamic responses based on user context.","Context-Driven API"
"A team is chaining MCP blocks for a complex system. What is a benefit?","The system processes data through multiple stages.","A) Modular design","B) Scalability","C) Reusability","D) All of the above","D","Chaining MCP blocks enables modular, scalable, and reusable system design.","MCP Chaining"
"A retail analyst is using scikit-learn to predict weekly sales. What is the best way to handle categorical features?","The dataset includes 'store_type' and 'region' columns.","A) Drop categorical features","B) One-hot encode categorical features","C) Use label encoding for all features","D) Ignore categorical features","B","One-hot encoding is preferred for non-ordinal categorical features in regression models.","ML Fundamentals"
"A data scientist is evaluating a regression model for house prices. Which metric best penalizes large errors?","The business wants to avoid large prediction mistakes.","A) MAE","B) RMSE","C) R2 Score","D) Accuracy","B","RMSE penalizes large errors more than MAE, making it suitable when large mistakes are costly.","Evaluation Metrics"
"A hospital wants to predict patient readmission risk. What is a key challenge in this ML task?","Readmission events are rare compared to non-readmissions.","A) Class imbalance","B) Feature scaling","C) Overfitting","D) Underfitting","A","Class imbalance is common in medical datasets with rare events.","ML Fundamentals"
"A startup is building a recommendation engine. What is collaborative filtering?","The system suggests products based on user-item interactions.","A) Uses user features only","B) Uses item features only","C) Uses similarities between users and items","D) Ignores user history","C","Collaborative filtering leverages similarities between users and items for recommendations.","Recommendation Systems"
"A bank is using logistic regression for credit approval. What does the output represent?","The model predicts approval (yes/no) for loan applications.","A) Probability of approval","B) Credit score","C) Loan amount","D) Interest rate","A","Logistic regression outputs the probability of the target class (approval).","Classification"
"A telecom company is clustering customers for marketing. What is the elbow method used for?","The team wants to choose the optimal number of clusters.","A) Evaluates cluster separation","B) Determines optimal K","C) Measures feature importance","D) Reduces dimensionality","B","The elbow method helps determine the optimal number of clusters (K) in clustering algorithms.","Clustering"
"A researcher is using PCA for dimensionality reduction. What is a benefit of PCA?","The dataset has 100 features, many of which are correlated.","A) Removes outliers","B) Reduces feature correlation","C) Improves model interpretability","D) All of the above","D","PCA reduces dimensionality, removes correlation, and can improve interpretability.","Dimensionality Reduction"
"A developer is training a neural network with PyTorch. What does optimizer.step() do?","The model is minimizing loss on training data.","A) Computes gradients","B) Updates model weights","C) Calculates loss","D) Loads data","B","optimizer.step() updates model weights using computed gradients.","PyTorch Training"
"A student is building an RNN for time series forecasting. What is a key feature of RNNs?","The model predicts future values based on past sequences.","A) Handles sequential data","B) Ignores order of data","C) Only works for images","D) Requires one-hot encoding","A","RNNs are designed to handle sequential data and capture temporal dependencies.","RNNs"
"A team is tuning dropout rate in a neural network. What is the effect of increasing dropout?","The model is overfitting on training data.","A) Reduces overfitting","B) Increases training speed","C) Improves accuracy","D) No effect","A","Increasing dropout reduces overfitting by randomly dropping neurons during training.","Dropout Regularization"
"A machine learning engineer is experimenting with optimizers in PyTorch. What is the role of the backward() function?","The model is training on image data and needs to update weights based on loss.","A) Computes gradients for all parameters","B) Updates weights directly","C) Calculates loss","D) Loads data","A","backward() computes gradients for all parameters with requires_grad=True.","PyTorch Backward Pass"
"A researcher is tuning the learning rate for Adam optimizer in PyTorch. What effect does increasing the learning rate have?","The model's loss fluctuates and sometimes diverges during training.","A) Faster convergence but risk of instability","B) Slower convergence","C) No effect","D) Only affects backward pass","A","Higher learning rates can speed up convergence but may cause instability or divergence.","PyTorch Optimizers"
"A student is training an MLP and wants to visualize gradients. What tool can they use?","The model is implemented in PyTorch and training on MNIST data.","A) TensorBoard","B) Matplotlib","C) Pandas","D) Seaborn","A","TensorBoard can visualize gradients, weights, and other metrics during training.","MLP Visualization"
"A data scientist is applying L1 regularization to a model. What is the expected effect?","The model has many features, some of which may be irrelevant.","A) Encourages sparsity in weights","B) Increases model complexity","C) Reduces training speed","D) No effect","A","L1 regularization encourages sparsity, driving some weights to zero.","Regularization"
"A team is comparing ML and DL models on a small dataset. What is a likely outcome?","The dataset has only 500 samples and 10 features.","A) DL models outperform ML models","B) ML models outperform DL models","C) Both perform equally","D) DL models overfit","B","ML models often outperform DL models on small datasets due to less overfitting.","ML vs DL"
"A developer is building a CNN for image classification. What is the main advantage of using convolutional layers?","The model processes 32x32 pixel images.","A) Captures spatial hierarchies","B) Reduces overfitting","C) Increases training speed","D) No advantage","A","Convolutional layers capture spatial hierarchies and local patterns in images.","CNN Layers"
"A researcher is tuning hyperparameters for a DL model. What is the best strategy?","The model has many hyperparameters and training is slow.","A) Grid search","B) Random search","C) Bayesian optimization","D) All of the above","D","All listed strategies can be used; Bayesian optimization is efficient for large search spaces.","Hyperparameter Tuning"
"A team is visualizing DL model performance. What metric is most informative for classification?","The model predicts 10 classes and outputs probabilities.","A) Accuracy","B) Confusion matrix","C) ROC-AUC","D) All of the above","D","Accuracy, confusion matrix, and ROC-AUC all provide valuable insights for classification models.","DL Performance"
"A data scientist is performing NLP preprocessing. What is an essential step before training?","Text data contains punctuation, stopwords, and mixed casing.","A) Tokenization","B) Removing stopwords","C) Lowercasing","D) All of the above","D","All listed steps are essential for cleaning text data before NLP model training.","NLP Preprocessing"
"A developer is building a sentiment classification model. What is the target variable?","The model predicts positive, negative, or neutral sentiment from text.","A) Continuous value","B) Categorical label","C) Probability score","D) Text length","B","Sentiment classification is a categorical prediction problem.","Sentiment Classification"
"A researcher is comparing BERT and GPT for attention visualization. What is a key difference?","Both models are used for NLP tasks.","A) BERT uses bidirectional attention, GPT uses unidirectional","B) Both use bidirectional attention","C) Both use unidirectional attention","D) Neither uses attention","A","BERT uses bidirectional attention, while GPT uses unidirectional (causal) attention.","BERT vs GPT"
"A team is using HuggingFace to finetune a transformer model. What is a common first step?","The model is loaded from the HuggingFace library.","A) Load pretrained weights","B) Train from scratch","C) Use random initialization","D) Skip tokenization","A","Loading pretrained weights is a common first step for finetuning.","HuggingFace Finetuning"
"A developer is scraping news headlines for NLP tasks. What is a key preprocessing step?","Headlines contain HTML tags and special characters.","A) Remove HTML tags","B) Tokenize text","C) Remove special characters","D) All of the above","D","All listed steps are important for cleaning scraped text data.","Text Scraping"
"A data scientist is applying regex for noisy text. What is a common use case?","Text data contains phone numbers and email addresses.","A) Extract patterns","B) Remove stopwords","C) Tokenize sentences","D) Lowercase text","A","Regex is commonly used to extract patterns like phone numbers and emails from text.","Regex in NLP"
"A researcher is applying quantization to a DL model. What is the expected benefit?","The model is deployed on edge devices with limited resources.","A) Reduces model size and inference time","B) Increases accuracy","C) Increases training time","D) No benefit","A","Quantization reduces model size and speeds up inference, especially on edge devices.","Quantization"
"A team is building a compressed LLM using QLoRA. What is a key advantage?","The model is trained with limited GPU resources.","A) Lower memory usage","B) Higher accuracy","C) Slower training","D) No advantage","A","QLoRA enables training large models with lower memory requirements.","QLoRA"
"A developer is deploying a model and testing endpoints. What is a common tool for endpoint testing?","The model is served via a REST API.","A) Postman","B) Jupyter Notebook","C) Excel","D) TensorBoard","A","Postman is widely used for testing REST API endpoints.","Model Deployment"
"A data scientist is starting training and partial deployment. What is a benefit of partial deployment?","The model is deployed to a subset of users for feedback.","A) Early feedback","B) Reduced risk","C) Faster iteration","D) All of the above","D","Partial deployment allows early feedback, reduces risk, and enables faster iteration.","Partial Deployment"
"A team is integrating and benchmarking model performance. What is a key metric for benchmarking?","The model is used for real-time predictions.","A) Latency","B) Throughput","C) Accuracy","D) All of the above","D","Latency, throughput, and accuracy are all important for benchmarking model performance.","Benchmarking"
"A developer is calling APIs for text, vision, and speech tasks. What is a common challenge?","The APIs return data in different formats.","A) Data normalization","B) Error handling","C) Rate limiting","D) All of the above","D","Data normalization, error handling, and rate limiting are common challenges when calling multiple APIs.","API Integration"
"A researcher is creating tool-based LangChain agents. What is a key feature of these agents?","The agent uses external tools to answer queries.","A) Tool integration","B) Memory management","C) Data visualization","D) Model training","A","Tool-based agents integrate external tools to enhance capabilities.","LangChain Agents"
"A team is using agents with memory and tools. What is a benefit of agent memory?","The agent interacts with users over multiple sessions.","A) Context retention","B) Faster responses","C) Improved accuracy","D) All of the above","D","Agent memory enables context retention, faster responses, and improved accuracy.","Agent Memory"
"A developer is packaging agents with Docker. What is a key advantage?","The agent is deployed across multiple environments.","A) Portability","B) Scalability","C) Isolation","D) All of the above","D","Docker packaging provides portability, scalability, and isolation for agent deployment.","Docker Packaging"
"A data scientist is embedding documents and creating indexes. What is the purpose of indexing?","The system retrieves relevant documents for user queries.","A) Faster search","B) Improved accuracy","C) Reduced storage","D) All of the above","A","Indexing enables faster search and retrieval of relevant documents.","Document Indexing"
"A team is building and testing a RAG-based retriever. What is a key component?","The retriever combines retrieval and generation for QA tasks.","A) Retriever model","B) Generator model","C) Index","D) All of the above","D","RAG-based retrievers use a retriever, generator, and index for QA tasks.","RAG Retriever"
"A developer is deploying to Azure Blob and invoking functions. What is a benefit of using Azure Functions?","The system processes uploaded files automatically.","A) Serverless execution","B) Scalability","C) Cost efficiency","D) All of the above","D","Azure Functions provide serverless execution, scalability, and cost efficiency.","Azure Functions"
"A researcher is creating a coordinating agent system. What is a key challenge?","Multiple agents must work together to solve complex tasks.","A) Communication","B) Synchronization","C) Error handling","D) All of the above","D","Communication, synchronization, and error handling are key challenges in multi-agent systems.","Agent Coordination"
"A team is testing multi-agent deployment locally. What is a common tool for local deployment?","The agents are built in Python and need to be tested together.","A) Docker Compose","B) Kubernetes","C) Jupyter Notebook","D) Excel","A","Docker Compose is commonly used for local multi-agent deployment and orchestration.","Multi-Agent Deployment"
"A developer is running a model on CPU vs GPU and comparing performance. What is a typical result?","The model is a deep neural network for image classification.","A) GPU is faster for large models","B) CPU is faster for small models","C) Both perform equally","D) GPU is always slower","A","GPUs are optimized for parallel computation and are faster for large models.","CPU vs GPU"
"A researcher is using TensorRT for model inference acceleration. What is a key benefit?","The model is deployed for real-time inference.","A) Reduced latency","B) Increased throughput","C) Lower memory usage","D) All of the above","D","TensorRT provides reduced latency, increased throughput, and lower memory usage for inference.","TensorRT Acceleration"
"A developer is building a context-driven API block. What is a key feature?","The API block adapts responses based on user context.","A) Dynamic response generation","B) Static responses","C) No context awareness","D) Only supports GET requests","A","Context-driven APIs generate dynamic responses based on user context.","Context-Driven API"
"A team is creating 3 MCP blocks and chaining them. What is a benefit of chaining MCP blocks?","The system processes data through multiple stages for complex tasks.","A) Modular design","B) Scalability","C) Reusability","D) All of the above","D","Chaining MCP blocks enables modular, scalable, and reusable system design.","MCP Chaining"
"A computer vision engineer is building a PyTorch model to classify images. What method is used to compute the output of the model?","The model is a subclass of nn.Module and receives a batch of images as input.","A) backward()","B) fit()","C) forward()","D) predict()","C","The forward() method defines the computation performed at every call of the model.","PyTorch Forward Pass"
"A data scientist wants to implement a custom activation function in PyTorch. Where should this be defined?","The model uses nn.Module and requires a non-standard activation after the first layer.","A) In the __init__ method","B) In the forward method","C) As a separate function outside the class","D) In the optimizer","B","Custom activations are typically defined in the forward() method of nn.Module.","PyTorch Forward Pass"
"A team is training a neural network for sentiment analysis using PyTorch. What is the output of the forward pass?","The model processes a batch of tokenized sentences and outputs logits for each class.","A) Gradients","B) Loss value","C) Predicted logits","D) Optimizer state","C","The forward pass computes the predicted outputs (logits) for the input batch.","PyTorch Forward Pass"
"A researcher is debugging a PyTorch model and wants to inspect intermediate layer outputs. What is the best approach?","The model has several convolutional and linear layers.","A) Print outputs inside the forward() method","B) Print outputs in __init__","C) Print outputs after optimizer.step()","D) Print outputs in backward()","A","Printing inside forward() allows inspection of intermediate activations during the forward pass.","PyTorch Forward Pass"
"A developer is using nn.Sequential to build a feedforward network in PyTorch. How does nn.Sequential work during the forward pass?","The network consists of multiple layers chained together.","A) Applies layers in reverse order","B) Applies layers in sequence as defined","C) Skips non-linear layers","D) Only applies the first layer","B","nn.Sequential applies each layer in the order they are defined during the forward pass.","PyTorch Forward Pass"
"A student wants to use GPU acceleration for the forward pass in PyTorch. What must they do?","The model and input tensors are currently on CPU.","A) Use .cuda() or .to('cuda') on model and tensors","B) Change optimizer settings","C) Use .cpu() on tensors","D) No action needed","A","Moving the model and tensors to GPU with .cuda() or .to('cuda') enables GPU acceleration.","PyTorch Forward Pass"
"A team is building a multi-input model in PyTorch. How should the forward method be defined?","The model receives both image and text inputs for multimodal classification.","A) Accept only one input argument","B) Accept multiple input arguments and process them","C) Use only keyword arguments","D) Inputs must be concatenated before calling forward()","B","The forward() method can accept multiple arguments and process them as needed.","PyTorch Forward Pass"
"A researcher is implementing dropout in a PyTorch model. What happens during the forward pass in training mode?","The model uses nn.Dropout after each hidden layer.","A) All neurons are dropped","B) No neurons are dropped","C) Random subset of neurons are dropped","D) Dropout is ignored","C","Dropout randomly zeroes some neuron outputs during training to prevent overfitting.","PyTorch Forward Pass"
"A developer is using batch normalization in a PyTorch model. What is the effect during the forward pass?","The model uses nn.BatchNorm1d after linear layers.","A) Normalizes activations across the batch","B) Normalizes activations across features only","C) No effect during forward pass","D) Only affects backward pass","A","Batch normalization normalizes activations across the batch during the forward pass.","PyTorch Forward Pass"
"A student is building a PyTorch model for regression. What should the forward method return?","The model predicts a continuous value for each input sample.","A) Class probabilities","B) Logits","C) Continuous output values","D) Loss value","C","For regression tasks, the forward method should return continuous output values.","PyTorch Forward Pass"
"A financial analyst is preparing a dataset with missing values in the 'income' column before building a predictive model. What is the best preprocessing step?","The dataset has 10% missing values in 'income', which is a key feature for credit risk prediction.","A) Drop all rows with missing income","B) Fill missing income with mean value","C) Fill missing income with median value","D) Leave missing values as is","C","Median imputation is less sensitive to outliers and preserves the distribution for skewed data.","Data Preprocessing"
"A marketing team wants to encode the 'region' column (North, South, East, West) for a classification model. Which encoding method is most suitable?","The model is a decision tree and the 'region' column is categorical with no ordinal relationship.","A) Label encoding","B) One-hot encoding","C) Binary encoding","D) Frequency encoding","B","One-hot encoding is preferred for non-ordinal categorical variables in tree-based models.","Data Preprocessing"
"A data scientist is scaling features before applying K-means clustering. Why is this step important?","Features include age (18-80), income ($20K-$200K), and purchase count (0-500).","A) To improve model interpretability","B) To ensure all features contribute equally to distance calculations","C) To reduce overfitting","D) To increase training speed","B","Scaling ensures that features with larger ranges do not dominate the clustering algorithm's distance calculations.","Data Preprocessing"
"A healthcare dataset contains outliers in the 'blood_pressure' column. What preprocessing technique should be used?","Some patients have extremely high or low blood pressure readings due to measurement errors.","A) Remove all outliers","B) Apply log transformation","C) Use robust scaling","D) Ignore outliers","C","Robust scaling uses median and interquartile range, making it less sensitive to outliers.","Data Preprocessing"
"A retail company wants to balance the classes in a customer churn dataset before training a model. What is the best approach?","Churned customers are only 5% of the dataset.","A) Oversample minority class","B) Undersample majority class","C) Use SMOTE","D) All of the above","D","All listed methods can help balance classes; SMOTE is a popular technique for synthetic oversampling.","Data Preprocessing"
"A researcher is building a regression model and wants to handle multicollinearity among features. What should they do?","Some features are highly correlated, causing unstable coefficient estimates.","A) Remove highly correlated features","B) Apply PCA","C) Use regularization (Ridge/Lasso)","D) All of the above","D","Removing correlated features, applying PCA, or using regularization can all address multicollinearity.","Data Preprocessing"
"A telecom company is normalizing the 'call_duration' feature for a neural network. Which method is most appropriate?","Call durations range from 1 to 120 minutes.","A) Min-max scaling","B) Standardization","C) Log transformation","D) No scaling needed","A","Min-max scaling transforms features to a fixed range, which is suitable for neural networks.","Data Preprocessing"
"A bank wants to encode the 'account_type' feature (Savings, Checking, Business) for a logistic regression model. Which encoding is best?","The model is sensitive to feature scaling and the feature is categorical.","A) Label encoding","B) One-hot encoding","C) Ordinal encoding","D) Frequency encoding","B","One-hot encoding avoids introducing ordinal relationships where none exist.","Data Preprocessing"
"A data scientist is handling missing values in a time series dataset. What is the best imputation method?","The dataset contains daily sales data with occasional missing days.","A) Forward fill","B) Backward fill","C) Interpolation","D) Drop missing days","C","Interpolation estimates missing values based on surrounding data, preserving time series continuity.","Data Preprocessing"
"A startup is preparing text data for sentiment analysis. What preprocessing step is essential before model training?","Text data contains typos, punctuation, and stopwords.","A) Tokenization","B) Removing stopwords","C) Lowercasing text","D) All of the above","D","All listed steps are essential for cleaning and preparing text data for NLP models.","Data Preprocessing"
"Question,Scenario,OptionA,OptionB,OptionC,OptionD,CorrectAnswer,Explanation,Topic
"A data scientist at TechCorp needs to predict house prices based on historical sales data with known prices. What type of machine learning approach should they use?","TechCorp Real Estate wants to build an automated valuation model using 50,000 historical home sales with features like square footage, location, bedrooms, and sale prices.","A) Unsupervised learning with clustering","B) Supervised learning with regression","C) Reinforcement learning with rewards","D) Semi-supervised learning with partial labels","B","Since we have historical data with known target values (sale prices), this is a supervised learning problem. We're predicting continuous values (prices), making it a regression task.","ML Fundamentals"
"A retail company wants to predict customer churn using historical purchase and engagement data. Which ML model should they use?","The company has labeled data indicating which customers have churned in the past year.","A) Clustering","B) Regression","C) Classification","D) Reinforcement Learning","C","Churn prediction is a classification problem since the target is categorical (churned/not churned).","ML Fundamentals"
"A hospital wants to group patients based on symptoms and test results to discover new disease subtypes. What ML approach is best?","No prior labels exist for disease subtypes; the goal is to find natural groupings.","A) Supervised learning","B) Unsupervised clustering","C) Regression","D) Reinforcement learning","B","Unsupervised clustering is used to discover patterns or groups in unlabeled data.","ML Fundamentals"
"A bank is building a credit scoring model using customer financial history. What is the target variable type?","The model predicts whether a customer will default on a loan (yes/no).","A) Continuous","B) Categorical","C) Ordinal","D) Binary","D","Default prediction is a binary classification problem (yes/no).","ML Fundamentals"
"A startup wants to recommend products to users based on their browsing and purchase history. Which ML technique is most suitable?","The system should learn user preferences and suggest relevant products.","A) Regression","B) Clustering","C) Recommendation systems","D) Classification","C","Recommendation systems use collaborative filtering or content-based methods to suggest items.","ML Fundamentals"
"A logistics company wants to optimize delivery routes using real-time traffic data. Which ML paradigm is most appropriate?","The system should learn to make sequential decisions to minimize delivery time.","A) Supervised learning","B) Unsupervised learning","C) Reinforcement learning","D) Regression","C","Reinforcement learning is used for sequential decision-making problems.","ML Fundamentals"
"A university wants to predict student grades based on attendance, assignment scores, and exam results. What ML model should be used?","The target variable is the final grade (A, B, C, D, F).","A) Regression","B) Classification","C) Clustering","D) Reinforcement learning","B","Predicting letter grades is a classification problem.","ML Fundamentals"
"A telecom company wants to detect fraudulent calls using call metadata. What is the main challenge in this ML task?","Fraudulent calls are rare compared to legitimate calls.","A) Data imbalance","B) Feature scaling","C) Overfitting","D) Underfitting","A","Fraud detection often faces class imbalance due to the rarity of fraud cases.","ML Fundamentals"
"A car manufacturer wants to forecast vehicle sales for the next quarter. Which ML model should they use?","Historical sales data is available for the past 5 years.","A) Regression","B) Classification","C) Clustering","D) Reinforcement learning","A","Forecasting sales is a regression problem since the target is continuous.","ML Fundamentals"
"A social media platform wants to identify trending topics from user posts. What ML technique is most suitable?","The platform has millions of unlabeled posts and wants to discover emerging topics.","A) Clustering","B) Classification","C) Regression","D) Reinforcement learning","A","Clustering can group similar posts to identify trends without labeled data.","ML Fundamentals"
...additional case-driven MCQs for each topic...
"In the house price prediction model, which evaluation metric would be MOST appropriate for measuring model performance?","The model needs to minimize prediction errors and the business cares about both small and large prediction errors equally.","A) Accuracy and Precision","B) Mean Absolute Error (MAE) and Root Mean Square Error (RMSE)","C) F1-Score and ROC-AUC","D) Confusion Matrix and Classification Report","B","For regression problems like price prediction, MAE and RMSE are appropriate metrics. MAE gives average absolute error, while RMSE penalizes larger errors more heavily.","Evaluation Metrics"
"The real estate dataset has missing values in the 'garage_size' column (20% missing) and 'year_built' column (5% missing). What's the best preprocessing strategy?","Missing garage_size values might indicate no garage, while missing year_built values appear random and could significantly impact price predictions.","A) Drop all rows with any missing values","B) Fill garage_size with 0, year_built with median value","C) Fill all missing values with column means","D) Use forward-fill for all missing values","B","Domain knowledge suggests missing garage_size likely means no garage (0), while year_built missing values should be imputed with median to avoid skewing the distribution.","Data Preprocessing"
"For the house price model with 50,000 samples, what would be an appropriate train-validation-test split strategy?","The model needs hyperparameter tuning and final performance evaluation. The data spans 10 years (2013-2023) with potential temporal patterns.","A) Random 80-10-10 split","B) Temporal split: 2013-2019 train, 2020-2021 validation, 2022-2023 test","C) 90-5-5 split to maximize training data","D) Cross-validation only, no separate test set","B","Temporal splitting prevents data leakage and better reflects real-world deployment where the model predicts future prices based on historical data.","Train-Test Split"
"After training Linear Regression, Random Forest, and XGBoost models, the validation RMSE results are: LR: $45K, RF: $32K, XGB: $30K. Training RMSE: LR: $44K, RF: $15K, XGB: $28K. Which model should you choose?","The business prioritizes model interpretability but also wants reasonable accuracy. Production environment has latency constraints.","A) Linear Regression for best interpretability","B) Random Forest shows severe overfitting","C) XGBoost for best validation performance","D) Ensemble of all three models","C","XGBoost has the best validation RMSE ($30K) with a reasonable gap from training RMSE ($28K), indicating good generalization without severe overfitting like Random Forest.","Model Selection"
"E-commerce company wants to group customers for targeted marketing without predefined categories. They have customer data: age, income, purchase history, website behavior. What ML approach?","No existing customer segments are defined. The marketing team wants to discover natural groupings in customer behavior to create personalized campaigns.","A) Supervised classification with predefined labels","B) Unsupervised clustering (K-means or hierarchical)","C) Reinforcement learning with reward functions","D) Semi-supervised learning with some labeled customers","B","Since there are no predefined customer categories and the goal is to discover natural groupings, this is an unsupervised clustering problem.","ML Fundamentals"
"For customer clustering, how would you evaluate the quality of the clustering results?","You applied K-means with K=5 and want to validate if this is optimal and whether the clusters are well-separated and meaningful.","A) Accuracy and F1-score","B) Silhouette score and elbow method for inertia","C) Precision and recall","D) Mean squared error","B","For clustering evaluation, silhouette score measures how well-separated clusters are, while the elbow method helps determine optimal number of clusters (K).","Evaluation Metrics"
"Customer data has features: age (20-80), income ($20K-$200K), purchase_count (0-500), days_since_last_purchase (0-365). What preprocessing is needed before clustering?","K-means clustering is sensitive to feature scales. Without preprocessing, income values will dominate the distance calculations.","A) No preprocessing needed","B) Normalize/standardize all features to same scale","C) Apply log transformation only","D) Convert all features to categorical","B","K-means uses Euclidean distance, so features with larger scales (income) will dominate. Standardization ensures all features contribute equally to clustering.","Data Preprocessing"
"For fraud detection in credit card transactions, you have 1M transactions with 0.1% fraud rate. What's the primary challenge and appropriate model selection strategy?","Each transaction has 30 features (amount, merchant, location, time, etc.). False positives are costly (declined legitimate transactions), but false negatives are more costly (fraudulent transactions).","A) Class imbalance; use accuracy as primary metric","B) Class imbalance; use precision-recall and adjust threshold","C) Small dataset; use simple linear models","D) Too many features; use unsupervised learning","B","With 0.1% fraud rate, class imbalance is the main challenge. Precision-recall metrics and threshold tuning are more appropriate than accuracy for imbalanced datasets.","Model Selection"
"In fraud detection, Precision=0.85, Recall=0.75, F1=0.80. What does this mean for business impact?","Current model catches 75% of actual fraud cases. Of all transactions flagged as fraud, 85% are actually fraudulent. The business wants to minimize both missed fraud and false alarms.","A) 85% of fraudulent transactions are caught","B) 75% of fraudulent transactions are caught, 15% of flagged transactions are false alarms","C) Model has 80% accuracy","D) 85% accuracy with 75% coverage","B","Recall=0.75 means 75% of actual fraud is detected. Precision=0.85 means 15% of flagged transactions are false positives (legitimate transactions incorrectly flagged).","Evaluation Metrics"
"In a neural network for image classification, what is the purpose of the activation function in hidden layers?","Building a network to classify handwritten digits (0-9). Each image is 28x28 pixels. Without activation functions, stacked linear layers would be equivalent to a single linear layer.","A) To normalize input values","B) To introduce non-linearity and enable learning complex patterns","C) To reduce overfitting","D) To speed up training","B","Activation functions introduce non-linearity, allowing neural networks to learn complex, non-linear relationships. Without them, multiple layers would collapse to a single linear transformation.","Neural Networks"
"In forward propagation for digit classification, given input X (784 features) → Hidden Layer (128 neurons) → Output Layer (10 classes), what happens at each step?","Input: flattened 28x28 image. Hidden layer uses ReLU activation. Output layer uses softmax for probability distribution over 10 digit classes.","A) X → W1*X+b1 → ReLU → W2*H+b2 → Softmax → Probabilities","B) X → Softmax → W1*X+b1 → ReLU → Output","C) X → ReLU → Linear → Linear → Softmax","D) X → Normalize → W1*X → W2*X → Argmax","A","Forward propagation: Input X → Linear transformation (W1*X+b1) → ReLU activation → Linear transformation (W2*H+b2) → Softmax for probability distribution.","Neural Networks"
"During backpropagation in the digit classifier, gradients flow backward to update weights. If the loss is high, what happens to weight updates?","Model incorrectly predicts '3' for an image of '8' with high confidence. The cross-entropy loss is large, requiring significant weight adjustments.","A) Small weight updates due to high loss","B) Large weight updates proportional to gradient magnitude","C) No weight updates until loss decreases","D) Random weight updates regardless of loss","B","Large losses produce large gradients, leading to larger weight updates (scaled by learning rate). This helps the model correct significant errors more aggressively.","Neural Networks"
"For the 10-class digit classification problem, why is cross-entropy loss preferred over mean squared error?","Output layer produces probability distribution: [0.1, 0.05, 0.6, 0.15, 0.05, 0.02, 0.01, 0.01, 0.005, 0.005] for digit '2' (true class index 2).","A) Cross-entropy is faster to compute","B) Cross-entropy works better with softmax and provides better gradients for classification","C) MSE is only for regression problems","D) Cross-entropy requires less memory","B","Cross-entropy loss is designed for classification with softmax output. It provides better gradients and naturally handles probability distributions, while MSE can cause vanishing gradients.","Loss Functions"
"Comparing SGD, Adam, and RMSprop optimizers for training the digit classifier, which statement is most accurate?","Training on 60,000 MNIST images. SGD learning rate=0.01, Adam learning rate=0.001, RMSprop learning rate=0.001. Monitoring training loss and validation accuracy.","A) SGD always converges fastest","B) Adam adapts learning rates per parameter and often converges faster than SGD","C) RMSprop is only for RNNs","D) All optimizers perform identically","B","Adam combines momentum and adaptive learning rates per parameter, often leading to faster convergence than SGD. It maintains separate learning rates for each parameter based on historical gradients.","Optimizers"
"In PyTorch, creating a neural network for digit classification. Which architecture design choice would be most appropriate?","Using PyTorch nn.Module. Need 784 input features (28x28 pixels), hidden layers, and 10 output classes. Want to prevent overfitting on 60K training samples.","A) Single linear layer: nn.Linear(784, 10)","B) Deep network: nn.Linear(784, 512) → ReLU → Dropout → nn.Linear(512, 128) → ReLU → nn.Linear(128, 10)","C) Very wide network: nn.Linear(784, 10000) → ReLU → nn.Linear(10000, 10)","D) No hidden layers needed","B","A moderately deep network with dropout regularization can learn complex patterns while preventing overfitting. The architecture progressively reduces dimensions from 784→512→128→10.","Neural Networks"
"In PyTorch autograd, when you call loss.backward(), what exactly happens in the computational graph?","Training loop: output = model(x) → loss = criterion(output, target) → loss.backward() → optimizer.step(). The model has 3 layers with thousands of parameters.","A) Only the final layer weights are updated","B) Gradients are computed for all parameters with requires_grad=True using chain rule","C) Forward pass is repeated","D) Random gradients are assigned","B","loss.backward() uses automatic differentiation to compute gradients for all parameters with requires_grad=True by applying the chain rule through the computational graph.","Neural Networks"
"After 50 epochs of training the digit classifier: Training accuracy = 99.5%, Validation accuracy = 87%. What's happening and how to fix it?","Model: 3 hidden layers with 512 neurons each. No regularization. Learning rate = 0.001. Training loss decreasing, validation loss increasing after epoch 20.","A) Underfitting; increase model complexity","B) Overfitting; add dropout, reduce model size, or early stopping","C) Perfect performance; no changes needed","D) Bad data; collect more samples","B","Large gap between training (99.5%) and validation (87%) accuracy indicates overfitting. Solutions include dropout, smaller model, early stopping, or data augmentation.","Overfitting/Underfitting"
"To evaluate the digit classifier's real-world performance, which evaluation strategy is most comprehensive?","Have 60K training, 10K validation, 10K test samples. Model shows: Train=99%, Val=91%, Test=89%. Need to assess performance across different digit classes and error patterns.","A) Only report test accuracy (89%)","B) Confusion matrix, per-class precision/recall, and error analysis on test set","C) Only training accuracy (99%)","D) Average of train, val, test accuracies","B","Comprehensive evaluation includes confusion matrix to see which digits are confused, per-class metrics to identify problematic classes, and error analysis to understand failure modes.","Model Evaluation"
"When implementing gradient descent in PyTorch, why do you need optimizer.zero_grad() before loss.backward()?","Training loop without optimizer.zero_grad(): for batch in dataloader: output=model(x); loss=criterion(output,y); loss.backward(); optimizer.step(). Gradients seem to accumulate strangely.","A) To reset the model weights","B) To clear accumulated gradients from previous iterations","C) To normalize gradients","D) To save memory","B","PyTorch accumulates gradients by default. Without zero_grad(), gradients from previous iterations add up, leading to incorrect gradient magnitudes and poor training.","Neural Networks"
"In a CNN for image classification, what is the primary purpose of convolutional layers?","Building CNN for CIFAR-10 (32x32 RGB images, 10 classes). Need to extract features like edges, shapes, textures from images before classification.","A) To reduce the number of parameters only","B) To detect local features like edges and patterns using learnable filters","C) To flatten the image into a vector","D) To apply non-linear activation","B","Convolutional layers use learnable filters to detect local features (edges, shapes, patterns) while preserving spatial relationships and reducing parameters through weight sharing.","CNN Architecture"
"In the CIFAR-10 CNN, you have Conv2d(3, 32, kernel_size=3) followed by Conv2d(32, 64, kernel_size=3). What do these numbers represent?","Input: 32x32x3 RGB images. First conv layer needs to learn 32 different 3x3 filters from RGB input. Second layer learns 64 filters from the 32 feature maps.","A) 3 input channels, 32 output channels, 3x3 filter size; then 32 input channels, 64 output channels, 3x3 filter","B) 3 layers, 32 neurons, kernel size 3","C) 3 filters, 32 images, 3x3 pooling","D) RGB values, batch size, filter count","A","Conv2d(in_channels, out_channels, kernel_size): First layer takes 3 RGB channels, outputs 32 feature maps with 3x3 filters. Second layer takes 32 input features, outputs 64 feature maps.","CNN Architecture"
"Why do CNNs typically use MaxPooling layers between convolutional layers?","CIFAR-10 CNN architecture: Conv→ReLU→MaxPool→Conv→ReLU→MaxPool→FC. After each conv block, feature maps are getting large and computation is expensive.","A) To increase the number of parameters","B) To add non-linearity","C) To reduce spatial dimensions and provide translation invariance","D) To prevent overfitting only","C","MaxPooling reduces spatial dimensions (downsampling), decreases computation, provides translation invariance, and helps the network focus on the most important features in each region.","CNN Architecture"
"In your CIFAR-10 CNN, the final feature maps are 4x4x128. Before the fully connected layer, what operation is typically applied?","After conv layers: 4x4x128 feature maps. Need to connect to fully connected layer with 512 neurons for classification. Spatial structure is no longer needed.","A) Another convolution","B) Batch normalization","C) Flatten to convert 4x4x128 to 2048-dimensional vector","D) Average pooling only","C","Flatten operation converts the 3D feature maps (4×4×128 = 2048 values) into a 1D vector that can be fed into fully connected layers for final classification.","CNN Architecture"
"During CNN training on CIFAR-10, training accuracy reaches 95% but validation accuracy plateaus at 75%. What's the most likely issue and solution?","CNN with 5 conv layers, no regularization. Training for 100 epochs. Training loss keeps decreasing while validation loss starts increasing after epoch 30.","A) Underfitting; add more layers","B) Overfitting; add dropout, data augmentation, or reduce model complexity","C) Bad optimization; change learning rate","D) Perfect performance; no changes needed","B","Large gap between training (95%) and validation (75%) accuracy indicates overfitting. Solutions include dropout, data augmentation, batch normalization, or early stopping.","Overfitting/Underfitting"
"For data augmentation in CIFAR-10 training, which transformations would be most appropriate?","Original 32x32 images of planes, cars, birds, cats, etc. Want to increase dataset diversity while preserving class labels. Some transformations might change the class.","A) Random rotation, horizontal flip, color jittering","B) Vertical flip, 180° rotation, grayscale conversion","C) Random crop, Gaussian noise, normalization","D) All possible transformations","A","For CIFAR-10, horizontal flips and small rotations preserve class identity. Vertical flips would create unrealistic images (upside-down cars), and extreme rotations might change object orientation.","Data Preprocessing"
"In batch normalization for CNNs, what problem does it primarily solve?","Training deep CNN on CIFAR-10. Without batch norm: gradients vanish in early layers, training is slow and unstable. Learning rates need careful tuning.","A) Overfitting","B) Internal covariate shift and gradient flow problems","C) Memory usage","D) Computational cost","B","Batch normalization addresses internal covariate shift (changing distributions of layer inputs during training) and improves gradient flow, enabling faster and more stable training.","CNN Architecture"
"When fine-tuning a pre-trained ResNet-50 on CIFAR-10, what's the best strategy?","ResNet-50 pre-trained on ImageNet (1000 classes, 224x224 images). CIFAR-10 has 10 classes, 32x32 images. Limited computational resources and 5K training samples per class.","A) Train all layers from scratch with random weights","B) Freeze early layers, fine-tune later layers, replace final classifier","C) Only train the final classifier layer","D) Use identical architecture without modifications","B","Early layers learn general features (edges, shapes) that transfer well. Fine-tuning later layers and replacing the classifier allows adaptation to CIFAR-10 while leveraging pre-trained features.","Transfer Learning"
"In transfer learning, why might you use different learning rates for different parts of the network?","Fine-tuning pre-trained ResNet on CIFAR-10. Early layers have good general features, later layers need adaptation. Final classifier starts with random weights.","A) All layers should use the same learning rate","B) Pre-trained layers use lower learning rates, new layers use higher learning rates","C) Random learning rates work best","D) Learning rate doesn't matter in transfer learning","B","Pre-trained layers need small updates (low learning rate) to preserve learned features. New/fine-tuned layers need larger updates (higher learning rate) to adapt to the new task.","Transfer Learning"
"For RNN text classification, what is the vanishing gradient problem and how does LSTM address it?","Training RNN to classify movie reviews (sequences of 200-500 words). Vanilla RNN struggles to learn long-term dependencies. Gradients become very small in early time steps.","A) LSTM adds more parameters to solve vanishing gradients","B) LSTM uses gating mechanisms to control information flow and maintain long-term memory","C) LSTM uses different activation functions","D) LSTM eliminates gradients completely","B","LSTM uses forget, input, and output gates to selectively remember/forget information, allowing gradients to flow through long sequences without vanishing.","RNN/LSTM"
"In LSTM for sentiment analysis, what do the forget gate, input gate, and output gate control?","Processing movie review: 'The movie started well but the ending was disappointing.' LSTM needs to remember positive sentiment early, then update with negative sentiment.","A) Forget: old memory, Input: new information, Output: current hidden state","B) All gates control the same information","C) Gates only affect training speed","D) Gates are not important for performance","A","Forget gate decides what to discard from cell state, input gate controls what new information to store, output gate determines what parts of cell state to output as hidden state.","RNN/LSTM"
"When using LSTM for sequence-to-sequence tasks, what is teacher forcing?","Training English-to-French translation model. During training, for input 'Hello world' → 'Bonjour monde', the decoder should predict each French word sequentially.","A) Using the target sequence as decoder input during training instead of predicted tokens","B) Forcing the model to learn faster","C) Using only correct predictions","D) Training without supervision","A","Teacher forcing feeds the true target tokens as decoder input during training, providing stable gradients and faster convergence compared to using the model's own predictions.","RNN/LSTM"
"In a bidirectional LSTM for named entity recognition, why process text in both directions?","Identifying person names in: 'Mr. John Smith visited Paris.' Context from both sides helps: 'Mr.' suggests a person name follows, and 'visited' confirms John Smith is a person.","A) To double the training speed","B) To capture context from both past and future tokens for better predictions","C) To reduce memory usage","D) Bidirectional is always better","B","Bidirectional processing allows the model to use both left context (past) and right context (future) when making predictions, providing richer representations for tasks like NER.","RNN/LSTM"
"For handling variable-length sequences in RNN training, what is sequence padding and masking?","Training sentiment classifier on movie reviews with lengths: 50, 150, 300, 80 words. Batch processing requires fixed-length tensors, but we don't want padding to affect learning.","A) Padding adds special tokens to make sequences equal length, masking ignores padded positions in loss calculation","B) Padding and masking are the same thing","C) Only padding is needed","D) Variable lengths don't matter","A","Padding adds special tokens (usually 0) to make sequences the same length for batching. Masking ensures padded positions don't contribute to loss or gradient calculations.","Data Preprocessing"
"In attention mechanisms for sequence-to-sequence models, what problem does attention solve?","English-French translation: 'The quick brown fox jumps over the lazy dog' → 'Le renard brun rapide saute par-dessus le chien paresseux.' Standard seq2seq compresses all input info into final hidden state.","A) Attention speeds up training","B) Attention allows the decoder to focus on relevant parts of the input sequence for each output token","C) Attention reduces memory usage","D) Attention eliminates the need for RNNs","B","Attention allows the decoder to dynamically focus on different parts of the input sequence when generating each output token, solving the information bottleneck of fixed-size context vectors.","Attention Mechanism"
"In self-attention (as used in Transformers), how are queries, keys, and values computed?","Processing sentence: 'The cat sat on the mat.' Each word needs to attend to all other words to understand relationships and context.","A) Q, K, V are predefined matrices","B) Q, K, V are learned linear projections of the input embeddings","C) Q, K, V are the same as input embeddings","D) Only queries matter","B","Self-attention computes Q=XW_Q, K=XW_K, V=XW_V where X is input embeddings and W matrices are learned parameters, allowing flexible attention patterns.","Attention Mechanism"
"What is the main advantage of Transformers over RNNs for sequence processing?","Training on large datasets (millions of sentences) for language modeling. RNNs process sequentially (word by word), while Transformers can process all positions simultaneously.","A) Transformers use less memory","B) Transformers enable parallel processing of sequences and better capture long-range dependencies","C) Transformers are simpler to implement","D) RNNs are always better","B","Transformers process all sequence positions in parallel (vs. sequential RNN processing) and use attention to directly connect distant positions, enabling better parallelization and long-range modeling.","Transformers"
"In the Transformer architecture, what is the purpose of positional encodings?","Processing sentence: 'John loves Mary' vs. 'Mary loves John.' Word order completely changes meaning, but self-attention is permutation-invariant without position information.","A) To add randomness","B) To provide position information since attention is permutation-invariant","C) To reduce overfitting","D) Positional encoding is optional","B","Self-attention doesn't inherently understand word order. Positional encodings add position information to embeddings, allowing the model to distinguish between different word orders.","Transformers"
"In multi-head attention, why use multiple attention heads instead of single attention?","Analyzing sentence: 'The bank by the river was steep.' Multiple attention heads can focus on different relationships: syntactic (bank-steep), semantic (bank-river), etc.","A) Multiple heads reduce computation","B) Multiple heads allow the model to attend to different types of relationships simultaneously","C) Single head is always sufficient","D) Multiple heads prevent overfitting","B","Multiple attention heads allow the model to capture different types of relationships (syntactic, semantic, positional) in parallel, providing richer representations than single attention.","Transformers"
"What is the difference between encoder-only, decoder-only, and encoder-decoder Transformer architectures?","Three tasks: document classification (BERT-style), text generation (GPT-style), and machine translation (T5-style). Each requires different architectural choices.","A) No significant differences","B) Encoder-only for classification, decoder-only for generation, encoder-decoder for seq2seq tasks","C) All architectures work equally for all tasks","D) Only decoder-only models are useful","B","Encoder-only (BERT): bidirectional, good for classification. Decoder-only (GPT): causal/autoregressive, good for generation. Encoder-decoder: best for translation and seq2seq tasks.","Transformers"
"In BERT training, what is the masked language modeling (MLM) objective?","Pre-training BERT on large text corpus. Input: 'The [MASK] jumped over the fence.' BERT needs to predict the masked word using bidirectional context.","A) Predicting the next word in sequence","B) Predicting masked words using bidirectional context","C) Classifying sentence sentiment","D) Translating between languages","B","MLM randomly masks 15% of input tokens and trains the model to predict them using bidirectional context, enabling BERT to learn rich bidirectional representations.","BERT/GPT"
"What is the key difference between BERT and GPT in terms of training objectives?","BERT sees full context when predicting masked words: 'The cat [MASK] on the mat.' GPT only sees left context when predicting next word: 'The cat' → predict 'sat'.","A) BERT and GPT use identical training","B) BERT uses bidirectional masked LM, GPT uses unidirectional autoregressive LM","C) BERT is only for classification","D) GPT cannot be fine-tuned","B","BERT uses masked language modeling with bidirectional context. GPT uses autoregressive language modeling, predicting next tokens using only left context.","BERT/GPT"
"When fine-tuning BERT for text classification, what modifications are typically made?","Fine-tuning BERT for movie review sentiment classification. BERT outputs contextual embeddings for each token, but need single classification decision.","A) Replace all BERT layers","B) Add a classification head on top of [CLS] token representation","C) Only use the last layer","D) Freeze all BERT parameters","B","Add a linear classification layer that takes the [CLS] token's final representation as input. The [CLS] token is designed to represent the entire sequence for classification tasks.","BERT/GPT"
"In GPT, what does the autoregressive generation process look like?","Generating text with GPT starting with prompt: 'The weather today is'. Model needs to generate continuation word by word while maintaining coherence.","A) Generate all words simultaneously","B) Generate words sequentially, using previous tokens as context for next token prediction","C) Generate random words","D) Copy from training data","B","GPT generates text autoregressively: predict next token given all previous tokens, then add predicted token to sequence and repeat. Each prediction uses full left context.","BERT/GPT"
"What is the difference between GPT-1, GPT-2, and GPT-3 in terms of scale and capabilities?","Evolution from GPT-1 (117M parameters) to GPT-3 (175B parameters). Each version shows emergent capabilities with scale increases.","A) Only architectural differences","B) Primarily scale differences: GPT-1 (117M) → GPT-2 (1.5B) → GPT-3 (175B) parameters, with emergent abilities","C) Only training data differences","D) No significant differences","B","The GPT series primarily differs in scale: GPT-1 (117M), GPT-2 (1.5B), GPT-3 (175B) parameters. Larger models show emergent capabilities like few-shot learning and reasoning.","BERT/GPT"
"In word embeddings, what does the distributional hypothesis suggest?","Training word embeddings on large corpus. Words 'car' and 'automobile' appear in similar contexts: 'drive a ___', 'park the ___', '_ mechanic'.","A) Words with similar meanings appear in similar contexts","B) Word order doesn't matter","C) All words are equally similar","D) Context is irrelevant","A","Distributional hypothesis states that words appearing in similar contexts tend to have similar meanings. This principle underlies word embedding methods like Word2Vec and GloVe.","Word Embeddings"
"What is the main difference between Word2Vec's CBOW and Skip-gram models?","Training embeddings on sentence: 'The quick brown fox jumps.' CBOW uses context words to predict target. Skip-gram uses target word to predict context.","A) CBOW and Skip-gram are identical","B) CBOW predicts target from context, Skip-gram predicts context from target","C) CBOW is only for classification","D) Skip-gram doesn't use neural networks","B","CBOW (Continuous Bag of Words) predicts the target word from context words. Skip-gram predicts context words from the target word. Skip-gram works better for rare words.","Word Embeddings"
"In GloVe embeddings, what information is captured by the co-occurrence matrix?","Building embeddings from news corpus. Words 'president' and 'election' frequently appear together. 'Cat' and 'election' rarely co-occur in the same context windows.","A) Alphabetical order of words","B) Global word co-occurrence statistics across the entire corpus","C) Word frequencies only","D) Random associations","B","GloVe constructs a global word-word co-occurrence matrix capturing how often words appear together across the entire corpus, then factorizes it to learn embeddings.","Word Embeddings"
"What are the limitations of static word embeddings like Word2Vec compared to contextual embeddings?","Word 'bank' in two sentences: 'I deposited money at the bank' vs. 'The river bank was muddy.' Word2Vec gives same embedding for 'bank' in both contexts.","A) Static embeddings are always better","B) Static embeddings assign same vector to words regardless of context, missing polysemy","C) No significant differences","D) Contextual embeddings are always worse","B","Static embeddings assign fixed vectors to words, missing context-dependent meanings (polysemy). Contextual embeddings (like BERT) provide different representations based on context.","Word Embeddings"
"In contextual embeddings (like those from BERT), how do representations differ from static embeddings?","Processing 'bank' in different contexts with BERT: 'financial bank' vs. 'river bank'. BERT generates different embeddings based on surrounding context.","A) Contextual embeddings are random","B) Contextual embeddings provide different representations for the same word in different contexts","C) Contextual embeddings are identical to static embeddings","D) Context doesn't matter","B","Contextual embeddings generate different vector representations for the same word based on its context, capturing polysemy and context-dependent meanings that static embeddings miss.","Word Embeddings"
"What is a language model and how is it typically evaluated?","Training a language model to predict the next word in sequences like: 'The capital of France is ___'. Model assigns probability distributions over vocabulary.","A) Language models only do classification","B) Language models predict probability distributions over next tokens, evaluated using perplexity","C) Language models don't need evaluation","D) Only accuracy matters","B","Language models predict probability distributions over next tokens given context. Perplexity measures how well the model predicts a test set - lower perplexity indicates better predictive performance.","Language Models"
"In n-gram language models, what is the Markov assumption and why is it limiting?","Bigram model for 'The cat sat on the' predicts next word only from 'the'. Doesn't consider that the sentence started with 'cat sat', losing important context.","A) N-gram models use infinite context","B) Markov assumption limits context to n-1 previous tokens, missing long-range dependencies","C) Markov assumption is always optimal","D) Context doesn't matter in language modeling","B","N-gram models assume independence beyond n-1 tokens (Markov assumption), missing long-range dependencies. Neural language models can capture longer contexts.","Language Models"
"How do neural language models (like RNN-based or Transformer-based) improve over n-gram models?","Comparing models on sentence completion: 'Although the weather was terrible, the concert was ___'. N-gram misses long-range 'weather terrible' → 'concert good' relationship.","A) Neural models use larger vocabularies","B) Neural models can capture longer dependencies and learn distributed representations","C) N-gram models are always better","D) No significant differences","B","Neural language models can capture longer dependencies through hidden states/attention and learn dense distributed representations, overcoming the sparse, limited-context nature of n-gram models.","Language Models"
"What is the purpose of temperature in language model sampling?","GPT generating text with different temperature values: T=0.1 produces very predictable text, T=2.0 produces more random, creative text.","A) Temperature controls model size","B) Temperature controls randomness in sampling - lower T more deterministic, higher T more random","C) Temperature doesn't affect generation","D) Temperature only affects training","B","Temperature scales the logits before softmax sampling. Low temperature (T→0) makes sampling more deterministic (greedy), high temperature makes it more random and creative.","Language Models"
"In language model evaluation, what does perplexity measure and how does it relate to bits per character?","Model A: perplexity=50, Model B: perplexity=25 on same test set. Lower perplexity indicates better predictive performance.","A) Perplexity measures model size","B) Perplexity measures how surprised the model is by the test data - lower is better, related to compression efficiency","C) Higher perplexity is always better","D) Perplexity is unrelated to model quality","B","Perplexity measures average surprise - how uncertain the model is about predicting test data. Lower perplexity = better model. Related to compression: lower perplexity = fewer bits needed per character.","Language Models"
"What is Retrieval-Augmented Generation (RAG) and why is it useful?","Question-answering system needs to answer: 'What was the GDP of Japan in 2023?' The base language model was trained on data through 2021 and lacks recent information.","A) RAG only retrieves information","B) RAG combines retrieval of relevant documents with language model generation for up-to-date, factual responses","C) RAG replaces language models entirely","D) RAG is only for classification","B","RAG retrieves relevant documents from a knowledge base and provides them as context to a language model for generation, enabling access to current information beyond training data.","RAG Systems"
"In a RAG system, what is the typical pipeline for answering a user query?","User asks: 'What are the latest developments in quantum computing?' System needs to find relevant documents and generate an informed response.","A) Generate answer directly without retrieval","B) Query → retrieve relevant documents → augment prompt with documents → generate answer","C) Only retrieve documents without generation","D) Random document selection","B","RAG pipeline: 1) Encode user query, 2) Retrieve relevant documents using similarity search, 3) Augment the prompt with retrieved documents, 4) Generate answer using the augmented context.","RAG Systems"
"What role do vector databases play in RAG systems?","RAG system has 1M documents about various topics. User query 'climate change effects' needs to quickly find the most relevant documents for context.","A) Vector databases store only text","B) Vector databases store document embeddings for fast similarity search and retrieval","C) Vector databases are not needed in RAG","D) Vector databases only store metadata","B","Vector databases store high-dimensional embeddings of documents, enabling fast similarity search. When a query comes in, its embedding is compared to find the most relevant document embeddings.","Vector Databases"
"In vector similarity search, what are the trade-offs between exact and approximate methods?","Searching through 10M document embeddings (768 dimensions each) for query 'machine learning algorithms'. Need balance between speed and accuracy.","A) Exact search is always faster","B) Exact search is accurate but slow for large datasets; approximate methods (like LSH, FAISS) trade some accuracy for speed","C) Approximate methods are always more accurate","D) No trade-offs exist","B","Exact search guarantees finding the true nearest neighbors but is computationally expensive. Approximate methods (LSH, FAISS, HNSW) sacrifice some accuracy for significant speed improvements on large datasets.","Vector Databases"
"What is the difference between dense and sparse retrieval in RAG systems?","Two retrieval approaches for query 'neural network architectures': sparse uses TF-IDF/BM25 matching 'neural', 'network', 'architectures'. Dense uses embedding similarity.","A) Dense and sparse are identical","B) Sparse uses exact term matching (TF-IDF, BM25), dense uses semantic similarity in embedding space","C) Dense is always better","D) Sparse is always better","B","Sparse retrieval uses exact term matching (TF-IDF, BM25) and works well for keyword-based queries. Dense retrieval uses semantic embeddings and captures conceptual similarity beyond exact terms.","RAG Systems"
"How can you evaluate the performance of a RAG system?","RAG system answers questions about company policies. Need to measure both retrieval quality (finding right documents) and generation quality (accurate, helpful answers).","A) Only measure generation quality","B) Evaluate both retrieval metrics (precision, recall) and generation metrics (factual accuracy, relevance)","C) Only measure retrieval quality","D) Random evaluation is sufficient","B","RAG evaluation requires: 1) Retrieval metrics (precision@k, recall, MRR) measuring if relevant documents are found, 2) Generation metrics (factual accuracy, relevance, fluency) measuring answer quality.","RAG Systems"
"What is the Model Context Protocol (MCP) and what problem does it solve?","AI assistant needs to access multiple tools: file system, databases, APIs, calculators. Each tool has different interfaces and connection methods.","A) MCP is only for training models","B) MCP provides a standardized protocol for AI models to securely connect to and interact with various tools and data sources","C) MCP replaces all AI models","D) MCP is only for web browsing","B","MCP standardizes how AI models connect to external tools and resources (files, databases, APIs), providing a secure, consistent interface across different applications and environments.","MCP Protocol"
"In MCP, what are the key components: clients, servers, and tools?","AI application (client) wants to use a code editor (server) that provides tools like file reading, writing, and execution. Need standardized communication protocol.","A) All components are identical","B) Clients are AI applications, servers provide capabilities, tools are specific functions exposed by servers","C) Only clients matter","D) Components are randomly assigned","B","MCP architecture: Clients (AI applications) connect to Servers (capability providers) which expose Tools (specific functions like file operations, API calls) through a standardized protocol.","MCP Protocol"
"What are the security considerations when implementing MCP servers?","MCP server provides file system access to AI applications. Need to prevent unauthorized access to sensitive files while allowing legitimate operations.","A) Security is not important","B) Implement authentication, authorization, sandboxing, and audit logging for safe tool access","C) Allow unrestricted access","D) Security is automatic","B","MCP security requires: authentication (verify client identity), authorization (control access permissions), sandboxing (limit tool capabilities), and audit logging (track all operations).","MCP Protocol"
"How does MCP enable composability of AI tools and services?","Building AI assistant that needs: web search + document analysis + code execution + email sending. Each capability is provided by different MCP servers.","A) MCP prevents tool combination","B) MCP allows AI applications to connect to multiple specialized servers, combining different capabilities seamlessly","C) Only single tools are supported","D) Composability is not possible","B","MCP enables composability by allowing clients to connect to multiple servers simultaneously, combining different tools (search, analysis, execution) into comprehensive AI workflows.","MCP Protocol"
"What are the benefits of standardizing AI tool interfaces through MCP?","Currently: each AI tool has custom APIs. Developers must learn multiple interfaces. Tools can't easily work together across different AI applications.","A) Standardization creates more complexity","B) Standardization reduces integration effort, improves interoperability, and enables tool reuse across applications","C) Custom interfaces are always better","D) Standardization is unnecessary","B","MCP standardization reduces development effort (learn once, use everywhere), improves interoperability between tools, and enables tool ecosystem growth through consistent interfaces.","MCP Protocol"
"When deploying machine learning models to production, what are the key considerations for model serving?","Trained image classification model needs to serve 1000 requests/second with <100ms latency. Model file is 500MB. Need reliability and cost efficiency.","A) Only accuracy matters in production","B) Consider latency, throughput, resource usage, scalability, monitoring, and cost","C) Deployment is identical to training","D) Model performance doesn't matter","B","Production serving requires: low latency, high throughput, efficient resource usage, auto-scaling, health monitoring, error handling, and cost optimization while maintaining model accuracy.","LLM Deployment"
"What is the difference between batch and real-time inference in ML deployment?","E-commerce company needs: 1) Product recommendations for website users (immediate), 2) Monthly customer segmentation analysis (can wait hours).","A) Batch and real-time are identical","B) Real-time serves individual requests immediately, batch processes large datasets offline","C) Batch is always faster","D) Real-time is always better","B","Real-time inference serves individual requests with low latency requirements. Batch inference processes large datasets efficiently offline when immediate response isn't needed.","LLM Deployment"
"How do you handle model versioning and rollback in production ML systems?","Deployed model v1.2 serving traffic. Model v1.3 shows better accuracy in testing but causes increased latency in production. Need to safely deploy and potentially rollback.","A) Always deploy immediately without testing","B) Use canary deployments, A/B testing, and maintain rollback capabilities for safe model updates","C) Never update models","D) Rollback is not necessary","B","Safe model deployment uses: canary releases (gradual traffic shift), A/B testing (compare performance), blue-green deployments, and automated rollback procedures for production stability.","LLM Deployment"
"What are the challenges and solutions for serving large language models (LLMs) in production?","Serving 70B parameter model: high memory requirements, expensive GPU costs, variable request lengths, potential generation latency issues.","A) LLMs don't need special considerations","B) Use model quantization, batching, caching, distributed serving, and efficient hardware for large model deployment","C) LLMs cannot be deployed","D) Only small models work in production","B","LLM serving challenges: memory requirements, latency, cost. Solutions include: quantization (reduce precision), dynamic batching, KV caching, distributed inference, and specialized hardware.","LLM Deployment"
"How do you monitor and maintain model performance in production?","Deployed fraud detection model initially had 90% precision. After 6 months, precision dropped to 75%. Model hasn't changed, but fraud patterns evolved.","A) Model monitoring is unnecessary","B) Monitor data drift, model drift, performance metrics, and implement retraining pipelines","C) Models never degrade","D) Manual monitoring is sufficient","B","Production ML monitoring requires: data drift detection (input distribution changes), model drift monitoring (performance degradation), automated alerts, and retraining pipelines to maintain performance.","Production ML"
"What is A/B testing in the context of ML model deployment and why is it important?","Testing new recommendation algorithm: 50% users see recommendations from model A, 50% from model B. Measuring click-through rates to determine which performs better.","A) A/B testing is only for web design","B) A/B testing compares model performance on real user traffic to make data-driven deployment decisions","C) A/B testing slows down deployment","D) Random assignment is not important","B","A/B testing splits real traffic between model versions to compare performance metrics (accuracy, user engagement, business KPIs) and make evidence-based decisions about model deployment.","Production ML"
"How do you handle concept drift in production ML systems?","Customer behavior prediction model trained pre-COVID shows declining accuracy. Shopping patterns, economic factors, and preferences have shifted significantly.","A) Concept drift is not a real problem","B) Detect distribution changes, retrain models on recent data, and implement adaptive learning systems","C) Old models always work forever","D) Manual updates are sufficient","B","Concept drift occurs when the relationship between features and targets changes. Solutions: drift detection algorithms, automated retraining on recent data, and adaptive learning systems.","Production ML"
"What are the security considerations for ML models in production?","Deployed image classification API: adversarial examples could fool the model, training data might contain sensitive information, model could be stolen through API queries.","A) ML models have no security risks","B) Protect against adversarial attacks, data privacy breaches, model stealing, and implement secure API design","C) Security is only for traditional software","D) Open access is always better","B","ML security includes: adversarial robustness (defend against crafted inputs), data privacy (protect training data), model protection (prevent theft), and secure API design.","Production ML"
"In AutoML systems, what aspects of the machine learning pipeline can be automated?","Data scientist spends weeks on: feature engineering, model selection, hyperparameter tuning, architecture search. Want to automate repetitive tasks while maintaining quality.","A) AutoML cannot automate anything","B) AutoML can automate feature engineering, model selection, hyperparameter optimization, and architecture search","C) Only hyperparameters can be automated","D) Automation reduces model quality","B","AutoML automates: data preprocessing, feature engineering, model selection, hyperparameter optimization, neural architecture search, and model evaluation - reducing manual effort while maintaining performance.","Production ML"
"What is the difference between supervised, unsupervised, and reinforcement learning paradigms?","Three scenarios: 1) Email spam detection with labeled examples, 2) Customer segmentation without predefined groups, 3) Game-playing AI learning from wins/losses.","A) All paradigms are identical","B) Supervised uses labeled data, unsupervised finds patterns in unlabeled data, RL learns from rewards/penalties","C) Only supervised learning works","D) Labels are always available","B","Supervised learning uses input-output pairs. Unsupervised learning discovers patterns in data without labels. Reinforcement learning learns optimal actions through reward/penalty feedback.","ML Fundamentals"
"What is cross-validation and why is it important for model evaluation?","Training sentiment classifier on 10K reviews. Single train-test split might by chance put all positive reviews in training set and negative in test set.","A) Cross-validation is unnecessary","B) Cross-validation provides more robust performance estimates by training/testing on multiple data splits","C) Single split is always sufficient","D) Cross-validation reduces accuracy","B","Cross-validation trains/tests on multiple data splits, providing more reliable performance estimates, reducing variance from random splits, and better detecting overfitting.","Model Evaluation"
"What is the bias-variance tradeoff in machine learning?","Comparing simple linear regression vs. complex neural network on house price prediction: Linear model underfits (high bias), complex model overfits to training noise (high variance).","A) Bias and variance are unrelated","B) High bias models underfit, high variance models overfit; optimal models balance both","C) High bias is always better","D) High variance is always better","B","Bias-variance tradeoff: High bias (underfitting) misses true patterns. High variance (overfitting) captures noise. Optimal models balance both for best generalization.","ML Fundamentals"
"How do you choose between different machine learning algorithms for a given problem?","Problem: predict customer churn with 100K samples, 50 features, mix of categorical and numerical data. Need interpretable model with good performance.","A) Always use the most complex algorithm","B) Consider data size, feature types, interpretability needs, performance requirements, and computational constraints","C) Random selection works best","D) Only neural networks should be used","B","Algorithm selection depends on: dataset size, feature types, interpretability requirements, performance needs, computational budget, and training time constraints.","Model Selection"
"What is feature engineering and why is it important?","Raw e-commerce data: user_age=25, purchase_date='2024-01-15', item_price=49.99. Want to predict repeat purchases but need better features.","A) Raw features are always optimal","B) Feature engineering creates more informative features from raw data to improve model performance","C) Feature engineering is unnecessary","D) More features always help","B","Feature engineering transforms raw data into informative features: age groups, days_since_purchase, price_bins, interaction features - often improving model performance more than algorithm choice.","Data Preprocessing"
"What is regularization and how does it help prevent overfitting?","Training polynomial regression: without regularization, model perfectly fits training data (including noise) but fails on validation set.","A) Regularization increases overfitting","B) Regularization adds penalty terms to loss function to constrain model complexity and improve generalization","C) Regularization is only for neural networks","D) Regularization reduces training accuracy","B","Regularization (L1, L2, dropout) adds penalties for model complexity, constraining weights to prevent overfitting and improve generalization to unseen data.","Overfitting/Underfitting"
"How do you handle imbalanced datasets in classification?","Credit card fraud detection: 99.9% normal transactions, 0.1% fraud. Standard accuracy metric and equal class weights will bias toward majority class.","A) Imbalanced data is not a problem","B) Use appropriate metrics (precision/recall), resampling techniques, cost-sensitive learning, or ensemble methods","C) Ignore minority class","D) Only accuracy matters","B","Imbalanced data solutions: use precision/recall/F1 metrics, oversample minority class (SMOTE), undersample majority class, cost-sensitive learning, or ensemble methods.","Model Selection"
"What is ensemble learning and when is it beneficial?","Individual models: Decision Tree (85% accuracy), SVM (87% accuracy), Random Forest (89% accuracy). Combining them might achieve better performance than any single model.","A) Ensemble learning always reduces performance","B) Ensemble learning combines multiple models to achieve better performance than individual models","C) Only single models should be used","D) Ensemble learning is too complex","B","Ensemble learning combines multiple models (voting, bagging, boosting) to reduce variance, bias, and improve generalization beyond individual model performance.","Ensemble Methods"
"What is the difference between bagging and boosting ensemble methods?","Random Forest (bagging): trains multiple trees on different data subsets independently. AdaBoost (boosting): trains models sequentially, focusing on previous errors.","A) Bagging and boosting are identical","B) Bagging trains models independently in parallel, boosting trains models sequentially focusing on errors","C) Boosting is always better","D) Bagging is always better","B","Bagging (Bootstrap AGGregating) trains models independently on different data subsets. Boosting trains models sequentially, with each model focusing on previous models' errors.","Ensemble Methods"
"How do you evaluate and interpret machine learning model results?","Binary classifier results: TP=850, TN=9500, FP=100, FN=50. Need to understand model performance and business impact.","A) Only accuracy matters","B) Use multiple metrics (precision, recall, F1, ROC-AUC), confusion matrix, and consider business impact","C) Single metric is sufficient","D) Evaluation is unnecessary","B","Comprehensive evaluation uses: accuracy, precision, recall, F1-score, ROC-AUC, confusion matrix, and business-specific metrics to understand model performance and impact.","Model Evaluation"