Question,Scenario,OptionA,OptionB,OptionC,OptionD,CorrectAnswer,Explanation,Topic
"A company wants to build a Q&A system for their internal documentation but needs the most current information. Their documents change frequently. What approach should they use?","Company has 10,000 internal documents that are updated daily. Standard language models are trained on data from months ago and can't access current information.","A) Fine-tune a language model on the documents","B) Use Retrieval-Augmented Generation (RAG) with a live document index","C) Build a rule-based system","D) Use only pre-trained models","B","RAG allows the system to retrieve current documents and use them as context for generation, ensuring answers reflect the most up-to-date information without retraining.","RAG Systems"
"In a RAG system, what is the typical pipeline for processing a user query?","User asks: 'What is our company policy on remote work?' System needs to find relevant policy documents and generate a comprehensive answer.","A) Generate answer directly from model memory","B) Query encoding → document retrieval → context augmentation → answer generation","C) Only search documents without generation","D) Random document selection","B","RAG pipeline: 1) Encode user query into embeddings, 2) Retrieve relevant documents using similarity search, 3) Augment prompt with retrieved content, 4) Generate answer using enhanced context.","RAG Systems"
"What are the key components of a vector database in a RAG system?","RAG system needs to store and search through 1 million document embeddings (768 dimensions each) for fast similarity matching.","A) Only stores text documents","B) Stores high-dimensional embeddings with fast similarity search capabilities","C) Traditional SQL database","D) Simple file storage","B","Vector databases store high-dimensional embeddings and provide fast similarity search using methods like cosine similarity, enabling efficient retrieval of relevant documents.","Vector Databases"
"What is the difference between dense and sparse retrieval methods in RAG?","Query: 'machine learning optimization techniques'. Dense retrieval uses semantic embeddings, sparse retrieval uses keyword matching like TF-IDF or BM25.","A) Dense and sparse are identical","B) Dense uses semantic embeddings, sparse uses exact keyword/term matching","C) Dense is always better","D) Sparse is always better","B","Dense retrieval uses neural embeddings for semantic similarity. Sparse retrieval uses traditional IR methods (TF-IDF, BM25) for exact term matching. Both have complementary strengths.","RAG Systems"
"How do you handle multi-hop reasoning in RAG systems?","Question: 'Who is the CEO of the company that acquired Instagram?' Requires finding: Instagram was acquired by Meta → Meta's CEO is Mark Zuckerberg.","A) Single retrieval step is always sufficient","B) Use iterative retrieval or reasoning chains to gather information across multiple documents","C) Multi-hop reasoning is impossible","D) Only simple questions can be answered","B","Multi-hop reasoning requires iterative retrieval, reasoning chains, or graph-based approaches to connect information across multiple documents or reasoning steps.","RAG Systems"
"What is chunking in RAG systems and why is it important?","Long research paper (50 pages) needs to be stored in vector database. Most embedding models have token limits (512-4096 tokens), requiring document segmentation.","A) Store entire documents as single embeddings","B) Split documents into smaller chunks to fit embedding model limits and improve retrieval granularity","C) Chunking reduces retrieval quality","D) Only short documents work in RAG","B","Chunking splits long documents into smaller segments that fit embedding model limits, improve retrieval precision, and allow more relevant context selection.","RAG Systems"
"How do you evaluate the performance of a RAG system?","RAG system answers questions about company policies. Need to measure both whether it finds the right documents and whether it generates accurate answers.","A) Only measure generation quality","B) Evaluate both retrieval metrics (precision, recall) and generation quality (accuracy, relevance)","C) Only measure retrieval quality","D) Evaluation is unnecessary","B","RAG evaluation requires: retrieval metrics (precision@k, recall, MRR) to measure document finding accuracy, and generation metrics (factual accuracy, relevance) for answer quality.","RAG Systems"
"What is re-ranking in RAG systems and when is it useful?","Initial retrieval returns 100 potentially relevant documents. Need to select the top 5 most relevant ones for context augmentation to fit prompt limits.","A) Use documents in random order","B) Apply more sophisticated models to re-rank initially retrieved documents by relevance","C) Always use all retrieved documents","D) Re-ranking is unnecessary","B","Re-ranking applies more sophisticated models (cross-encoders, neural re-rankers) to improve the relevance ranking of initially retrieved documents before context augmentation.","RAG Systems"
"How do you handle conflicting information in retrieved documents for RAG?","Query about product pricing returns documents with different prices: Document A says $100, Document B says $120, Document C says $110.","A) Always use the first document","B) Implement conflict detection, source reliability weighting, and transparent uncertainty communication","C) Ignore conflicting information","D) Average all conflicting values","B","Handle conflicts through: conflict detection algorithms, source credibility weighting, temporal information (most recent), and transparent communication of uncertainty to users.","RAG Systems"
"What is the role of embedding models in RAG systems?","RAG system needs to convert both user queries and documents into comparable vector representations for similarity search.","A) Embeddings are only for storage","B) Embedding models encode queries and documents into vector space for semantic similarity computation","C) Random vectors work just as well","D) Embeddings are unnecessary","B","Embedding models convert text into high-dimensional vectors that capture semantic meaning, enabling similarity computation between queries and documents in vector space.","Vector Databases"
"What are some popular vector database solutions and their characteristics?","Building production RAG system with 10M documents, need fast similarity search, scalability, and persistence.","A) Only traditional databases work","B) Solutions include Pinecone, Weaviate, Chroma, FAISS, Qdrant - each with different performance and feature trade-offs","C) Vector databases don't exist","D) File storage is sufficient","B","Popular vector databases: Pinecone (managed), Weaviate (open-source), Chroma (lightweight), FAISS (Facebook), Qdrant (Rust-based) - each optimized for different use cases.","Vector Databases"
"How do you choose the right similarity metric for vector search?","Comparing document similarity using different metrics: cosine similarity, Euclidean distance, dot product. Documents are normalized embeddings.","A) All similarity metrics are identical","B) Choose based on embedding characteristics: cosine for normalized vectors, Euclidean for absolute distance, dot product for unnormalized","C) Random selection is fine","D) Only cosine similarity works","B","Metric selection depends on use case: cosine similarity for normalized embeddings (most common), Euclidean distance for absolute proximity, dot product for magnitude-sensitive comparisons.","Vector Databases"
"What is approximate nearest neighbor (ANN) search and why is it used in vector databases?","Exact search through 10M high-dimensional vectors takes too long for real-time applications. Need to balance speed and accuracy.","A) Exact search is always faster","B) ANN trades some accuracy for significant speed improvements using indexing structures like LSH or HNSW","C) Approximate search is always less accurate","D) Only exact search should be used","B","ANN algorithms (LSH, HNSW, IVF) provide fast approximate similarity search by sacrificing small amounts of accuracy for dramatic speed improvements on large datasets.","Vector Databases"
"How do you handle vector database scaling and performance optimization?","Vector database with growing dataset from 1M to 100M vectors, increasing query load, need to maintain sub-100ms response times.","A) Scaling is automatic","B) Use sharding, indexing optimization, caching, and horizontal scaling strategies","C) Performance cannot be optimized","D) Vertical scaling is the only option","B","Scaling strategies: horizontal sharding across nodes, optimized indexing (HNSW parameters), result caching, load balancing, and hardware optimization (GPU acceleration).","Vector Databases"
"What is the Model Context Protocol (MCP) and what problem does it solve?","AI assistant needs to access multiple tools: file system, database queries, API calls, calculations. Each tool has different interfaces and security requirements.","A) MCP is only for training models","B) MCP provides standardized protocol for AI models to securely connect to external tools and data sources","C) MCP replaces all existing APIs","D) MCP is only for web applications","B","MCP standardizes how AI models interact with external tools, providing secure, consistent interfaces for accessing files, databases, APIs, and other resources across different applications.","MCP Protocol"
"In MCP architecture, what are the roles of clients, servers, and tools?","AI application wants to use code editor capabilities like file reading, writing, syntax checking, and code execution through standardized interface.","A) All components are identical","B) Clients (AI apps) connect to servers (capability providers) that expose tools (specific functions)","C) Only clients are needed","D) Servers and tools are the same","B","MCP architecture: Clients are AI applications, Servers provide capabilities and expose Tools (specific functions like file operations), connected through standardized protocol.","MCP Protocol"
"What are the security considerations when implementing MCP servers?","MCP server provides file system access to AI applications. Need to prevent unauthorized access while allowing legitimate operations.","A) Security is not important for AI tools","B) Implement authentication, authorization, sandboxing, and audit logging","C) Allow unlimited access","D) Security is automatic","B","MCP security requires: user authentication, granular authorization controls, sandboxed execution environments, audit logging, and principle of least privilege access.","MCP Protocol"
"How does MCP enable composability and interoperability of AI tools?","Building AI assistant that needs web search + document analysis + code execution + email sending capabilities from different tool providers.","A) Tools cannot be combined","B) MCP allows seamless integration of multiple specialized tools through standardized interfaces","C) Each tool needs custom integration","D) Composability reduces functionality","B","MCP enables composability by providing standardized interfaces, allowing AI applications to combine multiple specialized tools seamlessly without custom integration work.","MCP Protocol"
"What are the benefits of standardizing AI tool interfaces through MCP?","Currently each AI tool has unique API. Developers must learn multiple interfaces. Tools can't easily work together across different AI platforms.","A) Custom interfaces are always better","B) Standardization reduces integration effort, improves interoperability, and enables tool ecosystem growth","C) Standardization creates complexity","D) Unique interfaces are necessary","B","MCP standardization benefits: reduced development effort, improved tool interoperability, easier ecosystem growth, consistent security models, and better developer experience.","MCP Protocol"
"How do you implement error handling and recovery in MCP systems?","MCP server experiences network failures, permission errors, or tool timeouts while AI application is performing multi-step workflow.","A) Errors should crash the system","B) Implement graceful error handling, retry mechanisms, and workflow recovery strategies","C) Ignore all errors","D) Error handling is automatic","B","MCP error handling includes: graceful failure modes, intelligent retry logic, partial workflow recovery, clear error communication, and fallback strategies.","MCP Protocol"
"What is the difference between synchronous and asynchronous tool execution in MCP?","AI application requests: 1) Quick file read (instant), 2) Large data processing (5 minutes), 3) External API call (variable time).","A) All operations should be synchronous","B) Synchronous for quick operations, asynchronous for long-running tasks with progress tracking","C) Only asynchronous operations are allowed","D) Timing doesn't matter","B","MCP supports both: synchronous execution for quick operations, asynchronous execution with progress tracking and cancellation for long-running tasks.","MCP Protocol"
"How do you handle versioning and backward compatibility in MCP protocol?","MCP server updated with new features. Older AI clients still need to work while newer clients can use enhanced capabilities.","A) Always break backward compatibility","B) Implement semantic versioning with graceful degradation and feature negotiation","C) Versioning is unnecessary","D) Only latest version should work","B","MCP versioning strategy: semantic versioning, capability negotiation, graceful degradation for older clients, and clear deprecation policies for protocol evolution.","MCP Protocol"
"What role does schema validation play in MCP implementations?","MCP tools exchange structured data like file metadata, API responses, database query results. Need to ensure data consistency and type safety.","A) Schema validation is unnecessary","B) Schema validation ensures type safety, data consistency, and protocol compliance","C) Any data format works","D) Validation slows down performance","B","Schema validation ensures: type safety, data format consistency, protocol compliance, early error detection, and better debugging capabilities in MCP implementations.","MCP Protocol"
"How do you monitor and debug MCP-enabled AI applications?","AI application uses multiple MCP tools in complex workflows. Need to track tool usage, performance, errors, and user interactions for debugging.","A) Monitoring is not needed","B) Implement comprehensive logging, metrics collection, distributed tracing, and debugging tools","C) Simple logs are sufficient","D) Debugging is impossible","B","MCP monitoring includes: structured logging, performance metrics, distributed tracing across tool calls, error tracking, and specialized debugging tools for AI workflows.","MCP Protocol"
"When deploying large language models, what are the key infrastructure considerations?","Deploying 70B parameter model for production use with 1000 concurrent users, sub-2 second response time requirements.","A) Infrastructure doesn't matter for LLMs","B) Consider GPU memory, inference optimization, scaling, caching, and cost management","C) CPU-only deployment is sufficient","D) Single machine deployment is optimal","B","LLM deployment requires: high-memory GPUs, inference optimization (quantization, batching), auto-scaling, response caching, and cost-effective resource management.","LLM Deployment"
"What is model quantization and how does it help in LLM deployment?","70B model requires 140GB GPU memory in FP16. Production environment has limited GPU resources and needs faster inference.","A) Quantization increases model size","B) Quantization reduces precision (FP16→INT8) to decrease memory usage and increase inference speed","C) Quantization only works for small models","D) Precision doesn't affect performance","B","Quantization reduces numerical precision (FP32→FP16→INT8) to significantly reduce memory requirements and inference latency while maintaining most model performance.","LLM Deployment"
"What is dynamic batching in LLM serving and why is it important?","LLM receives requests with varying input lengths: some 10 tokens, others 1000 tokens. Static batching wastes GPU resources on padding.","A) Batch size should be fixed","B) Dynamic batching optimizes GPU utilization by batching requests of similar lengths together","C) Batching reduces performance","D) Single request processing is optimal","B","Dynamic batching groups requests by similar lengths to minimize padding waste, maximize GPU utilization, and improve overall throughput in LLM serving.","LLM Deployment"
"What is KV caching in transformer inference and how does it optimize generation?","Generating 100-token response: without KV caching, each new token requires recomputing attention for all previous tokens. With caching, reuse previous computations.","A) KV caching slows down generation","B) KV caching stores previous attention computations to avoid redundant calculations","C) Caching is only for training","D) Memory usage doesn't matter","B","KV caching stores key-value pairs from previous attention computations, eliminating redundant calculations and significantly speeding up autoregressive text generation.","LLM Deployment"
"How do you handle variable-length sequences efficiently in LLM deployment?","Batch contains sequences: [50, 200, 800, 1500] tokens. Padding to max length (1500) wastes computation on shorter sequences.","A) Always pad to maximum length","B) Use techniques like sequence packing, dynamic batching, and attention masking","C) Process sequences individually","D) Truncate all sequences to same length","B","Efficient handling uses: sequence packing (combine short sequences), dynamic batching by length, attention masking for padding, and bucketing strategies.","LLM Deployment"
"What are the trade-offs between different LLM serving frameworks?","Choosing between TensorRT-LLM, vLLM, DeepSpeed, and HuggingFace Text Generation Inference for production deployment.","A) All frameworks are identical","B) Frameworks differ in optimization focus: latency vs throughput, ease of use vs performance, hardware support","C) Only one framework works","D) Framework choice doesn't matter","B","Framework trade-offs: TensorRT-LLM (NVIDIA optimization), vLLM (high throughput), DeepSpeed (large models), TGI (ease of use) - each optimized for different scenarios.","LLM Deployment"
"How do you implement load balancing for LLM services?","Multiple LLM replicas handling user requests. Need to distribute load efficiently while considering model loading time and GPU memory usage.","A) Random load distribution works best","B) Implement intelligent routing considering model warmup, GPU utilization, and request characteristics","C) Use only single instance","D) Load balancing is automatic","B","LLM load balancing considers: model warmup state, GPU memory utilization, request queue length, and intelligent routing to optimize response times and resource usage.","LLM Deployment"
"What is the difference between stateless and stateful LLM serving?","Chatbot scenario: stateless serving treats each message independently, stateful maintains conversation context across multiple turns.","A) Stateless and stateful are identical","B) Stateless treats requests independently, stateful maintains context across multiple interactions","C) Only stateless serving works","D) State management is automatic","B","Stateless serving processes each request independently (simpler scaling). Stateful serving maintains conversation context (better user experience) but requires session management.","LLM Deployment"
"How do you handle model updates and versioning in production LLM systems?","New model version shows improved performance in testing but needs safe deployment without disrupting existing users.","A) Immediately replace old model","B) Use canary deployments, A/B testing, and gradual rollout strategies","C) Never update models","D) Model updates are automatic","B","Safe model updates use: canary deployments (gradual traffic shift), A/B testing (performance comparison), blue-green deployments, and automated rollback capabilities.","LLM Deployment"
"What are the cost optimization strategies for LLM deployment?","Running multiple LLM models with varying usage patterns. GPU costs are high and need optimization without sacrificing performance.","A) Cost optimization is impossible","B) Use spot instances, model sharing, auto-scaling, and efficient batching","C) Always use the most expensive hardware","D) Cost doesn't matter","B","Cost optimization strategies: spot instances for non-critical workloads, model sharing across applications, intelligent auto-scaling, efficient batching, and resource scheduling.","LLM Deployment"
"How do you monitor LLM performance and health in production?","Production LLM system needs monitoring for response quality, latency, resource usage, and potential issues like model degradation.","A) Monitoring is unnecessary","B) Monitor latency, throughput, resource usage, output quality, and error rates","C) Only basic logging is needed","D) Manual monitoring is sufficient","B","LLM monitoring includes: response latency/throughput metrics, GPU/memory utilization, output quality assessment, error rate tracking, and model drift detection.","Production ML"
"What is model drift in production ML systems and how do you detect it?","Recommendation model trained on pre-pandemic data shows declining performance as user behavior patterns change significantly.","A) Model drift doesn't exist","B) Model drift occurs when data distributions change; detected through performance monitoring and statistical tests","C) Models never degrade","D) Drift detection is impossible","B","Model drift happens when input data or target relationships change over time. Detected through: performance metric monitoring, statistical distribution tests, and data quality checks.","Production ML"
"How do you implement A/B testing for ML models in production?","Testing new recommendation algorithm: need to compare performance against existing model while ensuring fair user assignment and statistical significance.","A) A/B testing is unnecessary for ML","B) Randomly assign users to model variants, measure key metrics, and use statistical tests for significance","C) Always deploy new models immediately","D) Use only offline evaluation","B","ML A/B testing: random user assignment to model variants, careful metric selection, statistical significance testing, and consideration of interaction effects.","Production ML"
"What is data drift and how does it differ from concept drift?","E-commerce model: data drift = change in customer age distribution, concept drift = same age groups but different purchasing behaviors due to economic changes.","A) Data drift and concept drift are identical","B) Data drift = input distribution changes, concept drift = relationship between inputs and outputs changes","C) Only data drift matters","D) Neither type of drift exists","B","Data drift: changes in input feature distributions. Concept drift: changes in the relationship between features and target variables. Both require different detection and mitigation strategies.","Production ML"
"How do you implement automated retraining pipelines for ML models?","Model performance degrades over time due to data drift. Need automated system to retrain on fresh data when performance drops below threshold.","A) Manual retraining is always better","B) Implement trigger-based retraining with data validation, model evaluation, and automated deployment","C) Models never need retraining","D) Retraining reduces performance","B","Automated retraining includes: performance threshold monitoring, data quality validation, automated training pipelines, model evaluation gates, and safe deployment procedures.","Production ML"
"What are the security considerations for ML models in production?","Deployed image classification API vulnerable to adversarial examples, model stealing attacks, and potential data poisoning.","A) ML models have no security risks","B) Consider adversarial attacks, model extraction, data poisoning, and privacy protection","C) Security is only for traditional software","D) Open models are always secure","B","ML security considerations: adversarial robustness, model extraction protection, training data privacy, input validation, and secure API design.","Production ML"
"How do you handle model explainability and interpretability in production?","Loan approval model needs to provide explanations for decisions due to regulatory requirements and fairness concerns.","A) Explainability is unnecessary","B) Implement feature importance, SHAP values, or attention visualization depending on model type","C) Black-box models are always acceptable","D) Explanations reduce accuracy","B","Model explainability techniques: feature importance scores, SHAP values, LIME explanations, attention visualization, and counterfactual examples depending on model architecture.","Production ML"
"What is federated learning and when might it be useful?","Multiple hospitals want to collaborate on ML model training but cannot share patient data due to privacy regulations.","A) Federated learning is only for mobile apps","B) Federated learning trains models across distributed data sources without centralizing sensitive data","C) Data must always be centralized","D) Privacy is not important","B","Federated learning enables collaborative model training across multiple parties while keeping data decentralized, useful for privacy-sensitive domains like healthcare and finance.","Production ML"
"How do you implement model governance and compliance in ML systems?","Financial services ML system must comply with regulations, maintain audit trails, and ensure fairness across demographic groups.","A) Governance is unnecessary","B) Implement model registries, audit logging, bias monitoring, and compliance frameworks","C) Compliance reduces model performance","D) Manual processes are sufficient","B","ML governance includes: model registries, audit trails, bias and fairness monitoring, regulatory compliance frameworks, and automated documentation systems.","Production ML"
"What is AutoML and how can it help in ML model development?","Data scientist spends weeks on feature engineering, model selection, and hyperparameter tuning. Want to automate repetitive tasks.","A) AutoML replaces all data scientists","B) AutoML automates feature engineering, model selection, and hyperparameter optimization","C) Manual work is always better","D) AutoML only works for simple problems","B","AutoML automates: data preprocessing, feature engineering, model architecture search, hyperparameter optimization, and model selection - reducing manual effort while maintaining quality.","Production ML"
"What are the limitations and challenges of AutoML systems?","AutoML produces high-performing model but it's a complex ensemble that's hard to interpret and deploy in resource-constrained environment.","A) AutoML has no limitations","B) Limitations include interpretability, computational complexity, domain-specific requirements, and deployment constraints","C) AutoML solves all ML problems","D) Performance is always optimal","B","AutoML limitations: complex models may lack interpretability, high computational requirements, limited domain expertise integration, and potential deployment challenges.","Production ML"
"How do you choose between different ML deployment patterns (batch vs real-time vs streaming)?","Three scenarios: monthly customer segmentation, fraud detection for transactions, personalized recommendations on website visits.","A) All scenarios need real-time processing","B) Batch for periodic analysis, real-time for immediate decisions, streaming for continuous data processing","C) Only batch processing is reliable","D) Deployment pattern doesn't matter","B","Deployment pattern selection: batch for periodic/non-urgent tasks, real-time for immediate response needs, streaming for continuous data processing and real-time updates.","Production ML"
"What is edge deployment for ML models and what are its benefits and challenges?","Mobile app needs to run image classification locally on device without internet connection, considering privacy and latency.","A) Edge deployment is always worse","B) Edge deployment runs models locally for reduced latency and privacy, but faces resource and update challenges","C) Cloud deployment is always better","D) Edge deployment is impossible","B","Edge deployment benefits: reduced latency, privacy protection, offline capability. Challenges: limited computational resources, model size constraints, update mechanisms.","Production ML"
"How do you implement continuous integration and deployment (CI/CD) for ML systems?","ML team develops models iteratively. Need automated testing, validation, and deployment pipeline to ensure quality and reduce manual errors.","A) CI/CD doesn't apply to ML","B) Implement automated testing, model validation, and gradual deployment with rollback capabilities","C) Manual deployment is safer","D) Testing reduces model performance","B","ML CI/CD includes: automated data and model testing, performance validation, staged deployment, monitoring integration, and automated rollback procedures.","Production ML"
"What are some emerging trends in machine learning and AI that might impact future systems?","Considering future ML system architecture: multimodal models, neuromorphic computing, quantum ML, edge AI, and federated learning trends.","A) ML technology is static","B) Trends include multimodal integration, edge computing, quantum algorithms, neuromorphic hardware, and federated approaches","C) Only current technology matters","D) Future trends are unpredictable","B","Emerging trends: multimodal foundation models, edge AI deployment, quantum machine learning, neuromorphic computing, federated learning, and automated ML workflows.","Production ML"